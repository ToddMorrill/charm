{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tm3229/miniconda3/envs/charm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ldc_data import load_ldc_data\n",
    "from charm.model.args import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_ldc_data(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out any unprocessed data\n",
    "df = df[df['processed'] == True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode labeled change point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point_df = df[['file_id', 'changepoints']]\n",
    "change_point_df = change_point_df.explode(column='changepoints')\n",
    "change_point_df = change_point_df.reset_index(drop=True)\n",
    "change_point_df = pd.concat((change_point_df, pd.json_normalize(change_point_df['changepoints'])), axis=1)\n",
    "change_point_df = change_point_df.drop(columns=['changepoints'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>impact_scalar</th>\n",
       "      <th>comment</th>\n",
       "      <th>annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0100053I</td>\n",
       "      <td>399.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Pre-change: Speakers were exchanging informati...</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0100053J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0100053L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M0100053R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M01000545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_id  timestamp  impact_scalar  \\\n",
       "0  M0100053I      399.0            2.0   \n",
       "1  M0100053J        NaN            NaN   \n",
       "2  M0100053L        NaN            NaN   \n",
       "3  M0100053R        NaN            NaN   \n",
       "4  M01000545        NaN            NaN   \n",
       "\n",
       "                                             comment  annotator  \n",
       "0  Pre-change: Speakers were exchanging informati...      212.0  \n",
       "1                                                NaN        NaN  \n",
       "2                                                NaN        NaN  \n",
       "3                                                NaN        NaN  \n",
       "4                                                NaN        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_point_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode utterance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_df = df.drop(columns=['changepoints'])\n",
    "utterance_df = utterance_df.explode('utterances')\n",
    "utterance_df = utterance_df.reset_index(drop=True)\n",
    "utterance_df = pd.concat((utterance_df, pd.json_normalize(utterance_df['utterances'])), axis=1)\n",
    "utterance_df = utterance_df.drop(columns=['utterances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_df.columns = ['file_id', 'split', 'anno_start', 'anno_end', 'url', 'status_in_corpora',\n",
    "       'data_type', 'release', 'processed', 'start', 'end', 'text',\n",
    "       'avg_logprob', 'no_speech_prob', 'audio_files', 'video_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file id, filter out utterances whose end time is less than anno_start and whose start time is greater than anno_end\n",
    "utterance_df = utterance_df[utterance_df['end'] >= utterance_df['anno_start']]\n",
    "utterance_df = utterance_df[utterance_df['start'] <= utterance_df['anno_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge change_point_df and utterance_df on file_id and then filter down to rows where the start time of the utterance is less than the change point and the end time of the utterance is greater than the change point\n",
    "merged_df = pd.merge(change_point_df, utterance_df, on='file_id')\n",
    "merged_df = merged_df[merged_df['start'] <= merged_df['timestamp']]\n",
    "merged_df = merged_df[merged_df['end'] >= merged_df['timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data frame contains all the valid change points\n",
    "# left join back into utterance_df on file_id, start, and end\n",
    "merged_df = merged_df[['file_id', 'start', 'end', 'timestamp', 'impact_scalar', 'comment', 'annotator']]\n",
    "label_df = pd.merge(utterance_df, merged_df, on=['file_id', 'start', 'end'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we have all the expected change points\n",
    "assert label_df['timestamp'].notna().sum() == len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a labels column where 1 is a change point and 0 is not\n",
    "label_df['labels'] = label_df['timestamp'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values by file_id and start to be safe\n",
    "label_df = label_df.sort_values(by=['file_id', 'start'], ascending=True)\n",
    "label_df = label_df.reset_index(drop=True)\n",
    "# give ourselves an index to work with\n",
    "label_df = label_df.reset_index()\n",
    "# try to put an index on both the index and the file_id to speed up the slicing\n",
    "label_df = label_df.set_index(['index', 'file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, we can just retrieve the data point, determine which file_id it belongs to, slice the data frame, and then look at the [-k:idx+1] slice of the df\n",
    "idx = 4\n",
    "k = 3\n",
    "file_id = label_df.xs(idx, level=0, drop_level=True).index[0]\n",
    "start_idx = max(0, idx - k)\n",
    "# .loc[start_idx:idx] is inclusive\n",
    "utterances = label_df.xs(file_id, level=1, drop_level=False).loc[start_idx:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_file_path = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = tokenizer(\n",
    "            [label_df['text'].values.tolist()[:2]],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_attention_mask=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the pattern the model is using when concatenating multiple utterances is to\n",
    "# start the sequence with the cls_token_id, then finish an utterance with the eos_token_id,\n",
    "# then include a sep_token_id between utterances\n",
    "# and finally include an eos_token_id at the end of the sequence\n",
    "tokenizer.cls_token_id\n",
    "tokenizer.eos_token_id\n",
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "            label_df['text'].values.tolist()[:2],\n",
    "            add_special_tokens=False,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_attention_mask=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer.cls_token_id]\n",
    "for idx, utterance in enumerate(input_ids):\n",
    "    # add a sep token between utterances\n",
    "    if idx > 0:\n",
    "        tokens.append(tokenizer.sep_token_id)\n",
    "    tokens.extend(utterance)\n",
    "    tokens.append(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokens == expected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangePointDataset(Dataset):\n",
    "    \"\"\"Pretokenizes the text and combines window size utterances into one\n",
    "    sample, adding special tokens, as needed, when generating the example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, window_size=3, stride=1):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # pretokenize the text\n",
    "        # TODO: move over to an apache beam pipeline\n",
    "        # though there's not really an easy way to do this without replicating\n",
    "        # the data many times\n",
    "        self.df['input_ids'] = self.tokenizer(\n",
    "            self.df['text'].values.tolist(),\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_attention_mask=False)['input_ids']\n",
    "\n",
    "    def _get_tokens(self, input_id_list):\n",
    "        tokens = [self.tokenizer.cls_token_id]\n",
    "        for idx, utterance in enumerate(input_id_list):\n",
    "            # add a sep token between utterances\n",
    "            if idx > 0:\n",
    "                tokens.append(self.tokenizer.sep_token_id)\n",
    "            tokens.extend(utterance)\n",
    "            tokens.append(self.tokenizer.eos_token_id)\n",
    "        \n",
    "        # if the sequence is too long, truncate it starting from the beginning\n",
    "        # TODO: with this, you get the occasial sequence that starts with a sep token\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[-(self.max_len - 1):]\n",
    "            # add the cls token back\n",
    "            tokens = [self.tokenizer.cls_token_id] + tokens\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        # length is the number of examples that can be generated per filename\n",
    "        # times the number of filenames\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: speed this up somehow\n",
    "        file_id = self.df.xs(idx, level=0, drop_level=True).index[0]\n",
    "        start_idx = max(0, idx - self.window_size)\n",
    "        # .loc[start_idx:idx] is inclusive\n",
    "        utterances = self.df.xs(file_id, level=1, drop_level=False).loc[start_idx:idx]\n",
    "\n",
    "        input_id_list = utterances['input_ids'].values.tolist()\n",
    "        tokens = self._get_tokens(input_id_list)\n",
    "\n",
    "        # label should be the max label in the window (i.e. greedily label change points)\n",
    "        # i.e. if any of the utterances in the window are change points, then the window is a change point\n",
    "        label = utterances['labels'].max()\n",
    "        return {'input_ids': tokens, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point_dataset = ChangePointDataset(label_df, tokenizer, window_size=3, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time:  482.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# iterate over the dataset and confirm that the tokens/labels are what we expect\n",
    "start = time.perf_counter()\n",
    "data_points = []\n",
    "for i in range(len(change_point_dataset)):\n",
    "    data_points.append(change_point_dataset[i])\n",
    "    # if i > 100:\n",
    "    #     break\n",
    "end = time.perf_counter()\n",
    "print(f'elapsed time: {end - start: .2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.433333333333334"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# approx time (minutes) to iterate over the dataset\n",
    "(446) / 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics about the dataset\n",
    "- what's the min, max, median, mean length of the utterance\n",
    "- what truncation strategy should we employ (random sampling of the longest utterance? cut off the first couple utterances?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    2341\n",
       "Name: label_match, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(data_points) == len(label_df)\n",
    "label_df['encoded_data'] = data_points\n",
    "temp_df = pd.json_normalize(label_df['encoded_data'])\n",
    "temp_df.index = label_df.index\n",
    "label_df[['input_ids_final', 'label_final']] = temp_df[['input_ids', 'label']]\n",
    "check_labels_df = label_df[label_df['labels'] == 1][['labels', 'label_final']]\n",
    "check_labels_df['label_match'] = check_labels_df['labels'] == check_labels_df['label_final']\n",
    "check_labels_df['label_match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    430897.000000\n",
       "mean         71.769010\n",
       "std          14.832004\n",
       "min          13.000000\n",
       "25%          66.000000\n",
       "50%          70.000000\n",
       "75%          75.000000\n",
       "max         512.000000\n",
       "Name: input_ids_len, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get descriptive stats on the lengths of the input_ids\n",
    "label_df['input_ids_len'] = label_df['input_ids_final'].apply(lambda x: len(x))\n",
    "label_df['input_ids_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    430897.000000\n",
       "mean         54.067165\n",
       "std          11.625714\n",
       "min          13.000000\n",
       "25%          49.000000\n",
       "50%          53.000000\n",
       "75%          57.000000\n",
       "max         722.000000\n",
       "Name: input_ids_len, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take length of the utterance, add 2 for the cls and eos tokens, then sum windows within each file_id\n",
    "label_df['input_ids_len'] = label_df['input_ids'].apply(lambda x: len(x) + 2)\n",
    "label_df.groupby('file_id')['input_ids_len'].rolling(3, min_periods=1).sum().reset_index(drop=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    430897.000000\n",
       "mean          8.909350\n",
       "std           8.307804\n",
       "min           1.000000\n",
       "25%           5.000000\n",
       "50%           8.000000\n",
       "75%          11.000000\n",
       "max        1061.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df['text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(temp_df)):\n\u001b[1;32m      8\u001b[0m     start_idx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, i \u001b[39m-\u001b[39m (k\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m     utterances \u001b[39m=\u001b[39m temp_df\u001b[39m.\u001b[39;49mloc[start_idx:i]\n\u001b[1;32m     10\u001b[0m     label \u001b[39m=\u001b[39m utterances[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax()\n\u001b[1;32m     11\u001b[0m     \u001b[39m# if nan\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/charm/lib/python3.10/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/charm/lib/python3.10/site-packages/pandas/core/indexing.py:1289\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(key)\n\u001b[1;32m   1288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key(key, axis)\n\u001b[1;32m   1290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_slice_axis(key, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m   1291\u001b[0m \u001b[39melif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/charm/lib/python3.10/site-packages/pandas/core/indexing.py:1108\u001b[0m, in \u001b[0;36m_LocIndexer._validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1099\u001b[0m _valid_types \u001b[39m=\u001b[39m (\n\u001b[1;32m   1100\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlabels (MUST BE IN THE INDEX), slices of labels (BOTH \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mendpoints included! Can be slices of integers if the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mindex is integers), listlike of labels, boolean\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m )\n\u001b[1;32m   1105\u001b[0m \u001b[39m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# Key Checks\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m \u001b[39m@doc\u001b[39m(_LocationIndexer\u001b[39m.\u001b[39m_validate_key)\n\u001b[1;32m   1109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_key\u001b[39m(\u001b[39mself\u001b[39m, key, axis: \u001b[39mint\u001b[39m):\n\u001b[1;32m   1110\u001b[0m     \u001b[39m# valid for a collection of labels (we check their presence later)\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[39m# slice of labels (where start-end in labels)\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m     \u001b[39m# slice of integers (only if in the labels)\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# boolean not in slice and with boolean index\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m   1115\u001b[0m         is_bool_dtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1116\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mboolean\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1117\u001b[0m     ):\n\u001b[1;32m   1118\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m   1119\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m: boolean label can not be used without a boolean index\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# determine the class weights on the fly\n",
    "k = 4\n",
    "class_counts = [0, 0]\n",
    "for file_id in label_df[label_df['split'] == 'train'].index.get_level_values(1):\n",
    "    temp_df = label_df.xs(file_id, level=1)    \n",
    "    \n",
    "    for i in range(len(temp_df)):\n",
    "        start_idx = max(0, i - (k-1))\n",
    "        utterances = temp_df.loc[start_idx:i]\n",
    "        label = utterances['labels'].max()\n",
    "        # if nan\n",
    "        if np.isnan(label):\n",
    "            label = 0\n",
    "        class_counts[int(label)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = label_df[label_df['split'] == 'train']['label_final'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([301598,   6158])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0, 1]\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given this imbalance, we should probably use a weighted loss function\n",
    "# applying the following function: n_samples / (n_classes * np.bincount(y))\n",
    "loss_weights = class_counts.sum() / (len(class_counts) * class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51020895, 24.98830789])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in social orientation tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self, model_dir):\n",
    "        self.model_dir = model_dir\n",
    "        # self.model = model # just a blueprint\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.device = args.device\n",
    "    \n",
    "    def _get_latest_checkpoint(self):\n",
    "        # get the last checkpoint\n",
    "        checkpoints = [\n",
    "            f for f in os.listdir(self.args.model_dir) if 'checkpoint' in f\n",
    "            and os.path.isdir(os.path.join(self.args.model_dir, f))\n",
    "        ]\n",
    "        checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint = None\n",
    "        if len(checkpoints) > 0:\n",
    "            checkpoint = checkpoints[-1]\n",
    "        return checkpoint\n",
    "    \n",
    "    def load_config(self, checkpoint=None):\n",
    "        \"\"\"Loads the model from disk.\"\"\"\n",
    "        # if checkpoint is None, load best based on best_checkpoint.txt\n",
    "        if checkpoint is None:\n",
    "            with open(os.path.join(self.model_dir, 'best_checkpoint.txt'),\n",
    "                        'r') as f:\n",
    "                checkpoint = f.read()\n",
    "        elif checkpoint == 'last':\n",
    "            checkpoint = self._get_latest_checkpoint()\n",
    "            checkpoint = os.path.join(self.model_dir, checkpoint)\n",
    "        else:\n",
    "            checkpoint = os.path.join(self.model_dir, checkpoint)\n",
    "        \n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        # load trainer state\n",
    "        with open(os.path.join(checkpoint, 'trainer_state.json'), 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "            self.global_step = trainer_state['global_step']\n",
    "            self.epoch = trainer_state['epoch']\n",
    "            self.metrics = trainer_state['metrics']\n",
    "            self.wandb_run_id = trainer_state['wandb_run_id']\n",
    "            self.args = argparse.Namespace(**trainer_state['args'])\n",
    "        \n",
    "    def load_model(self, model):\n",
    "        self.model = model\n",
    "        # load model\n",
    "        # define device map so we load on rank 0 and broadcast to other ranks\n",
    "        # https://discuss.pytorch.org/t/checkpoint-in-multi-gpu/97852/11\n",
    "        map_location = None\n",
    "        # TODO: will need to adjust args to support this properly\n",
    "        if self.args.distributed:\n",
    "            map_location = f'cuda:{self.args.local_rank}'\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(os.path.join(self.checkpoint, 'model.pt'),\n",
    "                            map_location=map_location))\n",
    "            self.model.to(self.args.device)\n",
    "            logging.info(\n",
    "                f'Model device {self.model.device} on rank {self.args.local_rank}'\n",
    "            )\n",
    "            self.model = DDP(\n",
    "                self.model,\n",
    "                device_ids=[self.args.device],\n",
    "                output_device=self.args.device,\n",
    "            )\n",
    "            # dist.barrier()\n",
    "        else:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(os.path.join(self.checkpoint, 'model.pt'),\n",
    "                            map_location=map_location))\n",
    "            self.model.to(self.args.device)\n",
    "        \n",
    "        # put the model in eval mode\n",
    "        self.model.eval()\n",
    "        logging.info(f'Loaded model on {self.args.device}...')\n",
    "        # self.optimizer.load_state_dict(\n",
    "        #     torch.load(os.path.join(save_dir, 'optimizer.pt'),\n",
    "        #                 map_location=map_location))\n",
    "        # logging.info(f'Loaded optimizer on {self.args.device}...')\n",
    "        # self.lr_scheduler.load_state_dict(\n",
    "        #     torch.load(os.path.join(save_dir, 'lr_scheduler.pt'),\n",
    "        #                 map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg_list = [\n",
    "#         '--batch-size', '64',\n",
    "#         '--wandb-project', 'social-orientation',\n",
    "#         '--log-level', 'INFO',\n",
    "#         '--seed', '10',\n",
    "#         '--data-dir', '/mnt/swordfish-pool2/ccu/transformed/circumplex',\n",
    "#         '--model-dir', '/mnt/swordfish-pool2/ccu/models/xlm-roberta-base-pt']\n",
    "# args = parse_args(arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model_dir='/mnt/swordfish-pool2/ccu/models/xlm-roberta-base-pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.load_config(checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            predictor.args.model_name_or_path,\n",
    "            num_labels=len(predictor.args.label2id),\n",
    "            id2label=predictor.args.id2label,\n",
    "            label2id=predictor.args.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Predictor' object has no attribute 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda:2\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Predictor' object has no attribute 'args'"
     ]
    }
   ],
   "source": [
    "predictor.args.device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "predictor.load_model(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader from the label_df\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabelDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.df[['input_ids']].iloc[idx].to_dict()\n",
    "\n",
    "label_dataset = LabelDataset(label_df)\n",
    "collate = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "label_dataloader = DataLoader(label_dataset, batch_size=1024, num_workers=32, prefetch_factor=2, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(label_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/421 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 2/421 [00:37<1:49:28, 15.68s/it]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  1%|          | 3/421 [00:38<1:02:09,  8.92s/it]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 421/421 [06:26<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# make model predictions\n",
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for batch in tqdm(label_dataloader):\n",
    "        # move data to device\n",
    "        batch = {k: v.to(predictor.args.device) for k, v in batch.items()}\n",
    "        outputs = predictor.model(**batch)\n",
    "        logits = outputs[1]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predictions to label_df\n",
    "label_df['social_orientation_preds'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ids to ints\n",
    "predictor.args.id2label = {int(k): v for k, v in predictor.args.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['social_orientation_preds'] = label_df['social_orientation_preds'].map(predictor.args.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>@id</th>\n",
       "      <th>@start_char</th>\n",
       "      <th>@end_char</th>\n",
       "      <th>ORIGINAL_TEXT</th>\n",
       "      <th>@type</th>\n",
       "      <th>@begin_offset</th>\n",
       "      <th>@char_length</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>impact_scalar</th>\n",
       "      <th>comment</th>\n",
       "      <th>Utterance ID</th>\n",
       "      <th>Complete Line</th>\n",
       "      <th>Complete Line Length</th>\n",
       "      <th>line_len_cumsum</th>\n",
       "      <th>social_orientation</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>你手机充300不够是吧？</td>\n",
       "      <td>message</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>m0000</td>\n",
       "      <td>2012-07-09 08:54:39 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Speaker 1 (1):  你手机充300不够是吧？</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Unassured-Submissive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker 1 (1): Unassured-Submissive - The spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-1</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>115.51？</td>\n",
       "      <td>message</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>m0001</td>\n",
       "      <td>2012-07-09 08:56:50 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Speaker 2 (2):  115.51？</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Speaker 2 (2): Unassuming-Ingenuous - The spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-2</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>我都糊涂了</td>\n",
       "      <td>message</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>m0002</td>\n",
       "      <td>2012-07-09 08:56:53 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Speaker 2 (3):  我都糊涂了</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>Aloof-Introverted</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Speaker 2 (3): Aloof-Introverted - The speaker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-3</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>我打电话问问吧</td>\n",
       "      <td>message</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>m0003</td>\n",
       "      <td>2012-07-09 08:56:59 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Speaker 2 (4):  我打电话问问吧</td>\n",
       "      <td>15</td>\n",
       "      <td>63</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Speaker 2 (4): Unassuming-Ingenuous - The spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-4</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>昂</td>\n",
       "      <td>message</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>m0004</td>\n",
       "      <td>2012-07-09 08:57:56 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Speaker 1 (5):  昂</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>Unassured-Submissive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker 1 (5): Unassured-Submissive - The spea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename        @id  @start_char  @end_char ORIGINAL_TEXT  \\\n",
       "0  M01000EY0.ltf.xml  segment-0            0         11  你手机充300不够是吧？   \n",
       "1  M01000EY0.ltf.xml  segment-1           14         20       115.51？   \n",
       "2  M01000EY0.ltf.xml  segment-2           23         27         我都糊涂了   \n",
       "3  M01000EY0.ltf.xml  segment-3           30         36       我打电话问问吧   \n",
       "4  M01000EY0.ltf.xml  segment-4           39         39             昂   \n",
       "\n",
       "     @type  @begin_offset  @char_length     id                     time  ...  \\\n",
       "0  message              0            14  m0000  2012-07-09 08:54:39 UTC  ...   \n",
       "1  message             14             9  m0001  2012-07-09 08:56:50 UTC  ...   \n",
       "2  message             23             7  m0002  2012-07-09 08:56:53 UTC  ...   \n",
       "3  message             30             9  m0003  2012-07-09 08:56:59 UTC  ...   \n",
       "4  message             39             3  m0004  2012-07-09 08:57:56 UTC  ...   \n",
       "\n",
       "   impact_scalar comment Utterance ID                 Complete Line  \\\n",
       "0            NaN     NaN            1  Speaker 1 (1):  你手机充300不够是吧？   \n",
       "1            NaN     NaN            2       Speaker 2 (2):  115.51？   \n",
       "2            NaN     NaN            3         Speaker 2 (3):  我都糊涂了   \n",
       "3            NaN     NaN            4       Speaker 2 (4):  我打电话问问吧   \n",
       "4            NaN     NaN            5             Speaker 1 (5):  昂   \n",
       "\n",
       "   Complete Line Length  line_len_cumsum    social_orientation  utterance_id  \\\n",
       "0                    20               20  Unassured-Submissive           1.0   \n",
       "1                    12               32  Unassuming-Ingenuous           2.0   \n",
       "2                    16               48     Aloof-Introverted           3.0   \n",
       "3                    15               63  Unassuming-Ingenuous           4.0   \n",
       "4                     9               72  Unassured-Submissive           5.0   \n",
       "\n",
       "  speaker_id                                          label_str  \n",
       "0        1.0  Speaker 1 (1): Unassured-Submissive - The spea...  \n",
       "1        2.0  Speaker 2 (2): Unassuming-Ingenuous - The spea...  \n",
       "2        2.0  Speaker 2 (3): Aloof-Introverted - The speaker...  \n",
       "3        2.0  Speaker 2 (4): Unassuming-Ingenuous - The spea...  \n",
       "4        1.0  Speaker 1 (5): Unassured-Submissive - The spea...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load labeled circumplex data to sanity check these results\n",
    "# load the dataset\n",
    "circumplex_df = pd.read_csv(os.path.join(predictor.args.data_dir, 'gpt_labels_r1_mini_eval_text.csv'))\n",
    "circumplex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "circumplex_df['start'] = circumplex_df['@start_char'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>@id</th>\n",
       "      <th>@start_char</th>\n",
       "      <th>@end_char</th>\n",
       "      <th>ORIGINAL_TEXT</th>\n",
       "      <th>@type</th>\n",
       "      <th>@begin_offset</th>\n",
       "      <th>@char_length</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>comment</th>\n",
       "      <th>Utterance ID</th>\n",
       "      <th>Complete Line</th>\n",
       "      <th>Complete Line Length</th>\n",
       "      <th>line_len_cumsum</th>\n",
       "      <th>social_orientation</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>label_str</th>\n",
       "      <th>start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>你手机充300不够是吧？</td>\n",
       "      <td>message</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>m0000</td>\n",
       "      <td>2012-07-09 08:54:39 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Speaker 1 (1):  你手机充300不够是吧？</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Unassured-Submissive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker 1 (1): Unassured-Submissive - The spea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-1</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>115.51？</td>\n",
       "      <td>message</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>m0001</td>\n",
       "      <td>2012-07-09 08:56:50 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Speaker 2 (2):  115.51？</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Speaker 2 (2): Unassuming-Ingenuous - The spea...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-2</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>我都糊涂了</td>\n",
       "      <td>message</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>m0002</td>\n",
       "      <td>2012-07-09 08:56:53 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Speaker 2 (3):  我都糊涂了</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>Aloof-Introverted</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Speaker 2 (3): Aloof-Introverted - The speaker...</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-3</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>我打电话问问吧</td>\n",
       "      <td>message</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>m0003</td>\n",
       "      <td>2012-07-09 08:56:59 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Speaker 2 (4):  我打电话问问吧</td>\n",
       "      <td>15</td>\n",
       "      <td>63</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Speaker 2 (4): Unassuming-Ingenuous - The spea...</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-4</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>昂</td>\n",
       "      <td>message</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>m0004</td>\n",
       "      <td>2012-07-09 08:57:56 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Speaker 1 (5):  昂</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>Unassured-Submissive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker 1 (5): Unassured-Submissive - The spea...</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename        @id  @start_char  @end_char ORIGINAL_TEXT  \\\n",
       "0  M01000EY0.ltf.xml  segment-0            0         11  你手机充300不够是吧？   \n",
       "1  M01000EY0.ltf.xml  segment-1           14         20       115.51？   \n",
       "2  M01000EY0.ltf.xml  segment-2           23         27         我都糊涂了   \n",
       "3  M01000EY0.ltf.xml  segment-3           30         36       我打电话问问吧   \n",
       "4  M01000EY0.ltf.xml  segment-4           39         39             昂   \n",
       "\n",
       "     @type  @begin_offset  @char_length     id                     time  ...  \\\n",
       "0  message              0            14  m0000  2012-07-09 08:54:39 UTC  ...   \n",
       "1  message             14             9  m0001  2012-07-09 08:56:50 UTC  ...   \n",
       "2  message             23             7  m0002  2012-07-09 08:56:53 UTC  ...   \n",
       "3  message             30             9  m0003  2012-07-09 08:56:59 UTC  ...   \n",
       "4  message             39             3  m0004  2012-07-09 08:57:56 UTC  ...   \n",
       "\n",
       "   comment Utterance ID                 Complete Line Complete Line Length  \\\n",
       "0      NaN            1  Speaker 1 (1):  你手机充300不够是吧？                   20   \n",
       "1      NaN            2       Speaker 2 (2):  115.51？                   12   \n",
       "2      NaN            3         Speaker 2 (3):  我都糊涂了                   16   \n",
       "3      NaN            4       Speaker 2 (4):  我打电话问问吧                   15   \n",
       "4      NaN            5             Speaker 1 (5):  昂                    9   \n",
       "\n",
       "   line_len_cumsum    social_orientation utterance_id  speaker_id  \\\n",
       "0               20  Unassured-Submissive          1.0         1.0   \n",
       "1               32  Unassuming-Ingenuous          2.0         2.0   \n",
       "2               48     Aloof-Introverted          3.0         2.0   \n",
       "3               63  Unassuming-Ingenuous          4.0         2.0   \n",
       "4               72  Unassured-Submissive          5.0         1.0   \n",
       "\n",
       "                                           label_str  start  \n",
       "0  Speaker 1 (1): Unassured-Submissive - The spea...    0.0  \n",
       "1  Speaker 2 (2): Unassuming-Ingenuous - The spea...   14.0  \n",
       "2  Speaker 2 (3): Aloof-Introverted - The speaker...   23.0  \n",
       "3  Speaker 2 (4): Unassuming-Ingenuous - The spea...   30.0  \n",
       "4  Speaker 1 (5): Unassured-Submissive - The spea...   39.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circumplex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the label_df on file_id and start\n",
    "temp_df = pd.merge(label_df, circumplex_df[['file_id', 'start', 'social_orientation']], on=['file_id', 'start'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(temp_df) == len(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign the label_df so we retain the ground truth social orientation labels\n",
    "label_df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1082691/2715398218.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['social_orientation_match'] = temp_df['social_orientation_preds'] == temp_df['social_orientation']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3907206455203116"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the predictions, which is as expected\n",
    "temp_df = temp_df.dropna(subset=['social_orientation'])\n",
    "temp_df['social_orientation_match'] = temp_df['social_orientation_preds'] == temp_df['social_orientation']\n",
    "temp_df['social_orientation_match'].sum() / len(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label_df to disk\n",
    "data_dir = '/mnt/swordfish-pool2/ccu/transformed/change-point'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "label_df.to_csv(os.path.join(data_dir, 'change_point_social_orientation_train_val_test.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lost index when setting label_df = temp_df\n",
    "label_df = label_df.reset_index(drop=False)\n",
    "label_df = label_df.set_index(['index', 'file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>anno_start</th>\n",
       "      <th>anno_end</th>\n",
       "      <th>url</th>\n",
       "      <th>status_in_corpora</th>\n",
       "      <th>data_type</th>\n",
       "      <th>release</th>\n",
       "      <th>processed</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>...</th>\n",
       "      <th>video_frames</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>impact_scalar</th>\n",
       "      <th>comment</th>\n",
       "      <th>annotator</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>social_orientation_preds</th>\n",
       "      <th>social_orientation</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th>file_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>M01000538</th>\n",
       "      <td>train</td>\n",
       "      <td>402.8</td>\n",
       "      <td>721.7</td>\n",
       "      <td>na</td>\n",
       "      <td>[(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...</td>\n",
       "      <td>audio</td>\n",
       "      <td>LDC2022E18</td>\n",
       "      <td>True</td>\n",
       "      <td>402.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 5169, 155474, 133334, 9940, 378, 7614, 330...</td>\n",
       "      <td>Arrogant-Calculating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>还是挣那么多钱 [Arrogant-Calculating]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>M01000538</th>\n",
       "      <td>train</td>\n",
       "      <td>402.8</td>\n",
       "      <td>721.7</td>\n",
       "      <td>na</td>\n",
       "      <td>[(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...</td>\n",
       "      <td>audio</td>\n",
       "      <td>LDC2022E18</td>\n",
       "      <td>True</td>\n",
       "      <td>404.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 1036, 4, 631, 176923, 31183, 18350, 378, 7...</td>\n",
       "      <td>Warm-Agreeable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>对,我这边挺好的 [Warm-Agreeable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>M01000538</th>\n",
       "      <td>train</td>\n",
       "      <td>402.8</td>\n",
       "      <td>721.7</td>\n",
       "      <td>na</td>\n",
       "      <td>[(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...</td>\n",
       "      <td>audio</td>\n",
       "      <td>LDC2022E18</td>\n",
       "      <td>True</td>\n",
       "      <td>406.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 35168, 378, 106396, 66596, 214, 9, 116836,...</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>哦 [Unassuming-Ingenuous]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>M01000538</th>\n",
       "      <td>train</td>\n",
       "      <td>402.8</td>\n",
       "      <td>721.7</td>\n",
       "      <td>na</td>\n",
       "      <td>[(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...</td>\n",
       "      <td>audio</td>\n",
       "      <td>LDC2022E18</td>\n",
       "      <td>True</td>\n",
       "      <td>407.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[73675, 11973, 2391, 6147, 378, 106396, 66596,...</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>你注意点啊 [Unassuming-Ingenuous]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>M01000538</th>\n",
       "      <td>train</td>\n",
       "      <td>402.8</td>\n",
       "      <td>721.7</td>\n",
       "      <td>na</td>\n",
       "      <td>[(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...</td>\n",
       "      <td>audio</td>\n",
       "      <td>LDC2022E18</td>\n",
       "      <td>True</td>\n",
       "      <td>409.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 7064, 2183, 9449, 22191, 5070, 4502, 378, ...</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>没事写封信吧 [Unassuming-Ingenuous]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 split  anno_start  anno_end url  \\\n",
       "index file_id                                      \n",
       "0     M01000538  train       402.8     721.7  na   \n",
       "1     M01000538  train       402.8     721.7  na   \n",
       "2     M01000538  train       402.8     721.7  na   \n",
       "3     M01000538  train       402.8     721.7  na   \n",
       "4     M01000538  train       402.8     721.7  na   \n",
       "\n",
       "                                                 status_in_corpora data_type  \\\n",
       "index file_id                                                                  \n",
       "0     M01000538  [(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...     audio   \n",
       "1     M01000538  [(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...     audio   \n",
       "2     M01000538  [(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...     audio   \n",
       "3     M01000538  [(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...     audio   \n",
       "4     M01000538  [(LDC2022E11_CCU_TA1_Mandarin_Chinese_Developm...     audio   \n",
       "\n",
       "                    release  processed  start    end  ... video_frames  \\\n",
       "index file_id                                         ...                \n",
       "0     M01000538  LDC2022E18       True  402.0  404.0  ...           []   \n",
       "1     M01000538  LDC2022E18       True  404.0  406.0  ...           []   \n",
       "2     M01000538  LDC2022E18       True  406.0  407.0  ...           []   \n",
       "3     M01000538  LDC2022E18       True  407.0  409.0  ...           []   \n",
       "4     M01000538  LDC2022E18       True  409.0  410.0  ...           []   \n",
       "\n",
       "                 timestamp  impact_scalar comment annotator  labels  \\\n",
       "index file_id                                                         \n",
       "0     M01000538        NaN            NaN     NaN       NaN       0   \n",
       "1     M01000538        NaN            NaN     NaN       NaN       0   \n",
       "2     M01000538        NaN            NaN     NaN       NaN       0   \n",
       "3     M01000538        NaN            NaN     NaN       NaN       0   \n",
       "4     M01000538        NaN            NaN     NaN       NaN       0   \n",
       "\n",
       "                                                         input_ids  \\\n",
       "index file_id                                                        \n",
       "0     M01000538  [6, 5169, 155474, 133334, 9940, 378, 7614, 330...   \n",
       "1     M01000538  [6, 1036, 4, 631, 176923, 31183, 18350, 378, 7...   \n",
       "2     M01000538  [6, 35168, 378, 106396, 66596, 214, 9, 116836,...   \n",
       "3     M01000538  [73675, 11973, 2391, 6147, 378, 106396, 66596,...   \n",
       "4     M01000538  [6, 7064, 2183, 9449, 22191, 5070, 4502, 378, ...   \n",
       "\n",
       "                social_orientation_preds  social_orientation  \\\n",
       "index file_id                                                  \n",
       "0     M01000538     Arrogant-Calculating                 NaN   \n",
       "1     M01000538           Warm-Agreeable                 NaN   \n",
       "2     M01000538     Unassuming-Ingenuous                 NaN   \n",
       "3     M01000538     Unassuming-Ingenuous                 NaN   \n",
       "4     M01000538     Unassuming-Ingenuous                 NaN   \n",
       "\n",
       "                                     text_final  \n",
       "index file_id                                    \n",
       "0     M01000538  还是挣那么多钱 [Arrogant-Calculating]  \n",
       "1     M01000538       对,我这边挺好的 [Warm-Agreeable]  \n",
       "2     M01000538        哦 [Unassuming-Ingenuous]  \n",
       "3     M01000538    你注意点啊 [Unassuming-Ingenuous]  \n",
       "4     M01000538   没事写封信吧 [Unassuming-Ingenuous]  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1082691/1363681217.py:2: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reload_label_df = pd.read_csv(os.path.join(data_dir, 'change_point_social_orientation_train_val_test.csv'), index_col=[0, 1])\n"
     ]
    }
   ],
   "source": [
    "# load the label_df to understand what the index looks like\n",
    "reload_label_df = pd.read_csv(os.path.join(data_dir, 'change_point_social_orientation_train_val_test.csv'), index_col=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>anno_start</th>\n",
       "      <th>anno_end</th>\n",
       "      <th>url</th>\n",
       "      <th>status_in_corpora</th>\n",
       "      <th>data_type</th>\n",
       "      <th>release</th>\n",
       "      <th>processed</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>...</th>\n",
       "      <th>audio_files</th>\n",
       "      <th>video_frames</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>impact_scalar</th>\n",
       "      <th>comment</th>\n",
       "      <th>annotator</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>social_orientation_preds</th>\n",
       "      <th>social_orientation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430896</th>\n",
       "      <th>M01005FAG</th>\n",
       "      <td>train</td>\n",
       "      <td>34.6</td>\n",
       "      <td>334.6</td>\n",
       "      <td>https://www.bilibili.com/video/BV1uU4y187TL</td>\n",
       "      <td>[('LDC2023E03_CCU_TA1_Mandarin_Chinese_Develop...</td>\n",
       "      <td>video</td>\n",
       "      <td>LDC2022E18</td>\n",
       "      <td>True</td>\n",
       "      <td>333.68</td>\n",
       "      <td>335.32</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 3933, 57105, 243973, 107249]</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  split  anno_start  anno_end  \\\n",
       "       file_id                                  \n",
       "430896 M01005FAG  train        34.6     334.6   \n",
       "\n",
       "                                                          url  \\\n",
       "       file_id                                                  \n",
       "430896 M01005FAG  https://www.bilibili.com/video/BV1uU4y187TL   \n",
       "\n",
       "                                                  status_in_corpora data_type  \\\n",
       "       file_id                                                                  \n",
       "430896 M01005FAG  [('LDC2023E03_CCU_TA1_Mandarin_Chinese_Develop...     video   \n",
       "\n",
       "                     release  processed   start     end  ... audio_files  \\\n",
       "       file_id                                           ...               \n",
       "430896 M01005FAG  LDC2022E18       True  333.68  335.32  ...          []   \n",
       "\n",
       "                  video_frames  timestamp impact_scalar comment  annotator  \\\n",
       "       file_id                                                               \n",
       "430896 M01005FAG            []        NaN           NaN     NaN        NaN   \n",
       "\n",
       "                  labels                         input_ids  \\\n",
       "       file_id                                               \n",
       "430896 M01005FAG       0  [6, 3933, 57105, 243973, 107249]   \n",
       "\n",
       "                  social_orientation_preds  social_orientation  \n",
       "       file_id                                                  \n",
       "430896 M01005FAG      Unassuming-Ingenuous                 NaN  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_label_df.iloc[-1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revise the dataset class \n",
    "- optionally generate impact scalars\n",
    "- optionally include social orientation information in utterances\n",
    "- define the window to be forward and backward looking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['text_final'] = label_df['text'] + ' ' + label_df['social_orientation_preds'].apply(lambda x: f'[{x}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.loc[390:395]['impact_scalar'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangePointDataset(Dataset):\n",
    "    \"\"\"Pretokenizes the text and combines window size utterances into one\n",
    "    sample, adding special tokens, as needed, when generating the example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, window_size=3, impact_scalar=False, social_orientation=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "        self.window_size = window_size\n",
    "        self.impact_scalar = impact_scalar\n",
    "        self.social_orientation = social_orientation\n",
    "\n",
    "        # pretokenize the text\n",
    "        # TODO: move over to an apache beam pipeline\n",
    "        # though there's not really an easy way to do this without replicating\n",
    "        # the data many times\n",
    "        # TODO: add special tokens to the text\n",
    "        # TODO: use ground truth social orientation labels\n",
    "        if social_orientation:\n",
    "            # e.g. 还是挣那么多钱 [Arrogant-Calculating]\n",
    "            self.df['text_final'] = self.df['text'] + ' ' + self.df['social_orientation_preds'].apply(lambda x: f'[{x}]')\n",
    "        else:\n",
    "            self.df['text_final'] = self.df['text']\n",
    "        \n",
    "        self.df['input_ids'] = self.tokenizer(\n",
    "            self.df['text_final'].values.tolist(),\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_attention_mask=False)['input_ids']\n",
    "\n",
    "    def _get_tokens(self, input_id_list):\n",
    "        tokens = [self.tokenizer.cls_token_id]\n",
    "        for idx, utterance in enumerate(input_id_list):\n",
    "            # add a sep token between utterances\n",
    "            if idx > 0:\n",
    "                tokens.append(self.tokenizer.sep_token_id)\n",
    "            tokens.extend(utterance)\n",
    "            tokens.append(self.tokenizer.eos_token_id)\n",
    "        \n",
    "        # if the sequence is too long, truncate half from the beginning and half from the end\n",
    "        # TODO: with this, you get the occasial sequence that starts with a sep token\n",
    "        if len(tokens) > self.max_len:\n",
    "            overage = len(tokens) - self.max_len\n",
    "            tokens = tokens[((overage//2) + 2):-((overage//2) + 2)]\n",
    "            # add the cls and eos tokens back\n",
    "            tokens = [self.tokenizer.cls_token_id] + tokens + [self.tokenizer.eos_token_id]\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        # length is the number of examples that can be generated per filename\n",
    "        # times the number of filenames\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: speed this up somehow\n",
    "        file_id = self.df.xs(idx, level=0, drop_level=True).index[0]\n",
    "        start_idx = max(0, idx - self.window_size)\n",
    "        end_idx = min(len(self.df.xs(file_id, level=1, drop_level=False)), idx + (self.window_size - 1))\n",
    "        # .loc[start_idx:end_idx] is inclusive\n",
    "        utterances = self.df.xs(file_id, level=1, drop_level=False).loc[start_idx:end_idx]\n",
    "\n",
    "        input_id_list = utterances['input_ids'].values.tolist()\n",
    "        tokens = self._get_tokens(input_id_list)\n",
    "\n",
    "        # label should be the max label in the window (i.e. greedily label change points)\n",
    "        # i.e. if any of the utterances in the window are change points, then the window is a change point\n",
    "        label = utterances['labels'].max()\n",
    "        # if nan\n",
    "        if np.isnan(label):\n",
    "            label = 0\n",
    "        label = int(label)\n",
    "\n",
    "        # add the impact scalar if needed\n",
    "        if self.impact_scalar:\n",
    "            # get the min impact scalar in the window among the impact scalars that are not 0\n",
    "            # if impact scalar is not set it will NaN. Min will ignore NaNs if there is a non-NaN value\n",
    "            impact_scalar = utterances['impact_scalar'].min()\n",
    "            if np.isnan(impact_scalar):\n",
    "                impact_scalar = 0.0\n",
    "            return {'input_ids': tokens, 'label': label, 'impact_scalar': impact_scalar}\n",
    "        \n",
    "        return {'input_ids': tokens, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point_dataset = ChangePointDataset(label_df, tokenizer, window_size=3, impact_scalar=True, social_orientation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> 还是挣那么多钱 [Arrogant-Calculating]</s>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(change_point_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e61d9a2642b8024e4f17ac2cd34659c00d3c9b631aca0a7af51fb5dc57583c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
