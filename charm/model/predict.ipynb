{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tm3229/miniconda3/envs/charm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# reload on change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "from functools import cache\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from charm.model.model import XLMRClassificationPlusTripletLoss\n",
    "from charm.model.utils import get_data\n",
    "from charm.loaders.ldc_data import load_ldc_data\n",
    "from charm.model.average_precision import calculate_average_precision, filter_system_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model        change-point-social-orientation\n",
      "filtering                             lowest\n",
      "text                                0.242385\n",
      "video                               0.040131\n",
      "audio                               0.020504\n",
      "Name: 9, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# develop function to find best model\n",
    "predictions_dir = '/mnt/swordfish-pool2/ccu/predictions'\n",
    "modality='text'\n",
    "model_dirs = os.listdir(predictions_dir)\n",
    "# load val_results.json if present\n",
    "val_results = {}\n",
    "for model_dir in model_dirs:\n",
    "    val_results_filepath = os.path.join(predictions_dir, model_dir, 'val_results.json')\n",
    "    if os.path.exists(val_results_filepath):\n",
    "        with open(val_results_filepath, 'r') as f:\n",
    "            val_results[model_dir] = json.load(f)\n",
    "df = pd.DataFrame.from_dict(val_results, orient='index')#.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "df = df.stack().to_frame().rename(columns={0: 'val_results'}).reset_index().rename(columns={'level_0':'model', 'level_1': 'filtering'})\n",
    "df = pd.concat([df.drop(columns=['val_results']), pd.json_normalize(df['val_results'])], axis=1)\n",
    "# group by model and select filtering method with the highest text score\n",
    "best_df = df.groupby('model').apply(lambda x: x.loc[x[modality].idxmax()]).reset_index(drop=True)\n",
    "# go with change-point-medium-reweight for now\n",
    "best_row = best_df.iloc[best_df['text'].idxmax()]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_file_path = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1430371/2753180171.py:1: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(\n",
    "        '/mnt/swordfish-pool2/ccu/transformed/change-point', 'change_point_social_orientation_train_val_test.csv'),\n",
    "                     index_col=0)\n",
    "# split into train, val, and test sets\n",
    "train_df = df[df['split'] == 'train'].reset_index(drop=True).reset_index()\n",
    "val_df = df[df['split'] == 'val'].reset_index(drop=True).reset_index()\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True).reset_index()\n",
    "train_df = train_df.set_index(['index', 'file_id'])\n",
    "val_df = val_df.set_index(['index', 'file_id'])\n",
    "test_df = test_df.set_index(['index', 'file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.perf_counter()\n",
    "# idx_file_id_map = {idx: file_id for idx, file_id in train_df.index}\n",
    "# end = time.perf_counter()\n",
    "# print(f'idx_file_id_map took {end - start} seconds to create')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each file, get start and end indices\n",
    "# start = time.perf_counter()\n",
    "# file_id_start_end_df = {}\n",
    "# for file_id in train_df.index.get_level_values('file_id').unique():\n",
    "#     file_df = train_df.xs(file_id, level=1, drop_level=False)\n",
    "#     first_idx = file_df.iloc[0].name[0]\n",
    "#     last_idx = file_df.iloc[-1].name[0]\n",
    "#     file_id_start_end_df[file_id] = {'first_idx': first_idx, 'last_idx': last_idx, 'df': file_df}\n",
    "# end = time.perf_counter()\n",
    "# print(f'file_id_start_end took {end - start} seconds to create')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChangePointDataset(Dataset):\n",
    "#     \"\"\"Pretokenizes the text and combines window size utterances into one\n",
    "#     sample, adding special tokens, as needed, when generating the example.\n",
    "#     \"\"\"\n",
    "#     def __init__(self,\n",
    "#                  df,\n",
    "#                  tokenizer,\n",
    "#                  window_size=4,\n",
    "#                  impact_scalar=False,\n",
    "#                  social_orientation=False,\n",
    "#                  profile=False):\n",
    "#         self.df = df\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = tokenizer.model_max_length\n",
    "#         self.window_size = window_size\n",
    "#         self.impact_scalar = impact_scalar\n",
    "#         self.social_orientation = social_orientation\n",
    "#         self.profile = profile\n",
    "        \n",
    "#         # pretokenize the text\n",
    "#         # TODO: move over to an apache beam pipeline\n",
    "#         # though there's not really an easy way to do this without replicating\n",
    "#         # the data many times\n",
    "#         # TODO: add special tokens to the text\n",
    "#         # TODO: use ground truth social orientation labels\n",
    "#         if social_orientation:\n",
    "#             # e.g. 还是挣那么多钱 [Arrogant-Calculating]\n",
    "#             self.df['text_final'] = self.df['text'] + ' ' + self.df[\n",
    "#                 'social_orientation_preds'].apply(lambda x: f'[{x}]')\n",
    "#         else:\n",
    "#             self.df['text_final'] = self.df['text']\n",
    "\n",
    "#         self.df['input_ids'] = self.tokenizer(\n",
    "#             self.df['text_final'].values.tolist(),\n",
    "#             add_special_tokens=False,\n",
    "#             max_length=self.max_len,\n",
    "#             truncation=True,\n",
    "#             return_attention_mask=False)['input_ids']\n",
    "\n",
    "#         # build a map from index to file_id\n",
    "#         self.idx_file_id_map = {idx: file_id for idx, file_id in self.df.index}\n",
    "#         self.file_id_start_end_df = {}\n",
    "#         for file_id in self.df.index.get_level_values('file_id').unique():\n",
    "#             file_df = self.df.xs(file_id, level=1, drop_level=False)\n",
    "#             first_idx = file_df.iloc[0].name[0]\n",
    "#             last_idx = file_df.iloc[-1].name[0]\n",
    "#             self.file_id_start_end_df[file_id] = (first_idx, last_idx, file_df)\n",
    "        \n",
    "\n",
    "#     def _get_tokens(self, input_id_list):\n",
    "#         tokens = [self.tokenizer.cls_token_id]\n",
    "#         for idx, utterance in enumerate(input_id_list):\n",
    "#             # add a sep token between utterances\n",
    "#             if idx > 0:\n",
    "#                 tokens.append(self.tokenizer.sep_token_id)\n",
    "#             tokens.extend(utterance)\n",
    "#             tokens.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "#         # if the sequence is too long, truncate half from the beginning and half from the end\n",
    "#         # TODO: with this, you get the occasial sequence that starts with a sep token\n",
    "#         if len(tokens) > self.max_len:\n",
    "#             overage = len(tokens) - self.max_len\n",
    "#             tokens = tokens[((overage // 2) + 2):-((overage // 2) + 2)]\n",
    "#             # add the cls and eos tokens back\n",
    "#             tokens = [self.tokenizer.cls_token_id\n",
    "#                       ] + tokens + [self.tokenizer.eos_token_id]\n",
    "#         return tokens\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # length is the number of examples that can be generated per filename\n",
    "#         # times the number of filenames\n",
    "#         return len(self.df)\n",
    "\n",
    "#     @cache\n",
    "#     def __getitem__(self, idx):\n",
    "#         # TODO: speed this up somehow\n",
    "#         get_data_start = time.perf_counter()\n",
    "#         file_id = self.idx_file_id_map[idx]\n",
    "#         first_idx, last_idx, file_df = self.file_id_start_end_df[file_id]\n",
    "#         # turns out that using iloc is faster than using loc\n",
    "#         # translate the idx to a row number\n",
    "#         idx_row_num = idx - first_idx\n",
    "#         # get the start and end indices for the window\n",
    "#         start_idx = max(0, idx_row_num - self.window_size)\n",
    "#         end_idx = min(last_idx - first_idx, idx_row_num + (self.window_size - 1))\n",
    "#         get_data_end = time.perf_counter()\n",
    "#         get_data_duration = get_data_end - get_data_start\n",
    "\n",
    "#         get_slice_start = time.perf_counter()\n",
    "#         # .loc[start_idx:end_idx] is inclusive\n",
    "#         utterances = file_df.iloc[start_idx:end_idx+1]\n",
    "#         get_slice_end = time.perf_counter()\n",
    "#         get_slice_duration = get_slice_end - get_slice_start\n",
    "\n",
    "#         # print(f'utterances are {utterances}')\n",
    "#         convert_start = time.perf_counter()\n",
    "#         input_id_list = utterances['input_ids'].values.tolist()\n",
    "#         convert_end = time.perf_counter()\n",
    "#         convert_duration = convert_end - convert_start\n",
    "\n",
    "#         get_tokens_start = time.perf_counter()\n",
    "#         tokens = self._get_tokens(input_id_list)\n",
    "#         get_tokens_end = time.perf_counter()\n",
    "#         get_tokens_duration = get_tokens_end - get_tokens_start\n",
    "#         # print(f'tokens is {tokens}')\n",
    "\n",
    "#         remainder_start = time.perf_counter()\n",
    "#         # label should be the max label in the window (i.e. greedily label change points)\n",
    "#         # i.e. if any of the utterances in the window are change points, then the window is a change point\n",
    "#         label = utterances['labels'].max()\n",
    "#         # print(f'label is {label}')\n",
    "#         # if nan, then set to 0\n",
    "#         if np.isnan(label):\n",
    "#             label = 0\n",
    "#         label = int(label)\n",
    "\n",
    "#         # add the impact scalar if needed\n",
    "#         if self.impact_scalar:\n",
    "#             # get the min impact scalar in the window among the impact scalars that are not 0\n",
    "#             # if impact scalar is not set it will NaN. Min will ignore NaNs if there is a non-NaN value\n",
    "#             impact_scalar = utterances['impact_scalar'].min()\n",
    "#             if np.isnan(impact_scalar):\n",
    "#                 impact_scalar = 0.0\n",
    "#             return {\n",
    "#                 'input_ids': tokens,\n",
    "#                 'label': label,\n",
    "#                 'impact_scalar': impact_scalar\n",
    "#             }\n",
    "#         remainder_end = time.perf_counter()\n",
    "#         remainder_duration = remainder_end - remainder_start\n",
    "#         if self.profile:\n",
    "#             time_dict = {'get_data_duration': get_data_duration, 'get_slice_duration': get_slice_duration, 'convert_duration': convert_duration, 'get_tokens_duration': get_tokens_duration, 'remainder_duration': remainder_duration}\n",
    "#             return {'input_ids': tokens, 'label': label}, time_dict\n",
    "#         return {'input_ids': tokens, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ChangePointDataset(train_df, tokenizer, window_size=10, social_orientation=False, impact_scalar=False, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.perf_counter()\n",
    "# time_dict = {}\n",
    "# for i in range(10000):\n",
    "#     item, times = train_dataset[i]\n",
    "#     for k, v in times.items():\n",
    "#         if k not in time_dict:\n",
    "#             time_dict[k] = 0\n",
    "#         time_dict[k] += v\n",
    "# end = time.perf_counter()\n",
    "# print(f'elapsed time is {end - start}')\n",
    "# print(f'time_dict is {time_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[395][0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self, model_dir):\n",
    "        self.model_dir = model_dir\n",
    "        # self.model = model # just a blueprint\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.device = args.device\n",
    "    \n",
    "    def _get_latest_checkpoint(self):\n",
    "        # get the last checkpoint\n",
    "        checkpoints = [\n",
    "            f for f in os.listdir(self.args.model_dir) if 'checkpoint' in f\n",
    "            and os.path.isdir(os.path.join(self.args.model_dir, f))\n",
    "        ]\n",
    "        checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint = None\n",
    "        if len(checkpoints) > 0:\n",
    "            checkpoint = checkpoints[-1]\n",
    "        return checkpoint\n",
    "    \n",
    "    def load_config(self, checkpoint=None):\n",
    "        \"\"\"Loads the model from disk.\"\"\"\n",
    "        # if checkpoint is None, load best based on best_checkpoint.txt\n",
    "        if checkpoint is None:\n",
    "            with open(os.path.join(self.model_dir, 'best_checkpoint.txt'),\n",
    "                        'r') as f:\n",
    "                checkpoint = f.read()\n",
    "        elif checkpoint == 'last':\n",
    "            checkpoint = self._get_latest_checkpoint()\n",
    "            checkpoint = os.path.join(self.model_dir, checkpoint)\n",
    "        else:\n",
    "            checkpoint = os.path.join(self.model_dir, checkpoint)\n",
    "        \n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        # load trainer state\n",
    "        with open(os.path.join(checkpoint, 'trainer_state.json'), 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "            self.global_step = trainer_state['global_step']\n",
    "            self.epoch = trainer_state['epoch']\n",
    "            self.metrics = trainer_state['metrics']\n",
    "            self.wandb_run_id = trainer_state['wandb_run_id']\n",
    "            self.args = argparse.Namespace(**trainer_state['args'])\n",
    "        \n",
    "    def load_model(self, model):\n",
    "        self.model = model\n",
    "        # load model\n",
    "        # define device map so we load on rank 0 and broadcast to other ranks\n",
    "        # https://discuss.pytorch.org/t/checkpoint-in-multi-gpu/97852/11\n",
    "        map_location = None\n",
    "        # TODO: will need to adjust args to support this properly\n",
    "        if self.args.distributed:\n",
    "            map_location = f'cuda:{self.args.local_rank}'\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(os.path.join(self.checkpoint, 'model.pt'),\n",
    "                            map_location=map_location))\n",
    "            self.model.to(self.args.device)\n",
    "            logging.info(\n",
    "                f'Model device {self.model.device} on rank {self.args.local_rank}'\n",
    "            )\n",
    "            self.model = DDP(\n",
    "                self.model,\n",
    "                device_ids=[self.args.device],\n",
    "                output_device=self.args.device,\n",
    "            )\n",
    "            # dist.barrier()\n",
    "        else:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(os.path.join(self.checkpoint, 'model.pt'),\n",
    "                            map_location=map_location))\n",
    "            self.model.to(self.args.device)\n",
    "        \n",
    "        # put the model in eval mode\n",
    "        self.model.eval()\n",
    "        logging.info(f'Loaded model on {self.args.device}...')\n",
    "        # self.optimizer.load_state_dict(\n",
    "        #     torch.load(os.path.join(save_dir, 'optimizer.pt'),\n",
    "        #                 map_location=map_location))\n",
    "        # logging.info(f'Loaded optimizer on {self.args.device}...')\n",
    "        # self.lr_scheduler.load_state_dict(\n",
    "        #     torch.load(os.path.join(save_dir, 'lr_scheduler.pt'),\n",
    "        #                 map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model_dir='/mnt/swordfish-pool2/ccu/models/change-point-class-reweight')\n",
    "predictor.load_config(checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRClassificationPlusTripletLoss: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRClassificationPlusTripletLoss from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRClassificationPlusTripletLoss from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRClassificationPlusTripletLoss were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLMRClassificationPlusTripletLoss.from_pretrained(\n",
    "            'xlm-roberta-base',\n",
    "            num_labels=len(predictor.args.id2label),\n",
    "            id2label=predictor.args.id2label,\n",
    "            label2id=predictor.args.label2id)\n",
    "# this will enable things like triplet loss, impact scalar, social orientation, and class weighting\n",
    "model.add_args(predictor.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.args.device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.load_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/swordfish-pool2/todd/charm/charm/model/utils.py:233: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "train_loader, val_loader, test_loader, args = get_data(predictor.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/484 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 484/484 [02:51<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# make model predictions\n",
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    llr = []\n",
    "    labels = []\n",
    "    for batch in tqdm(val_loader):\n",
    "        labels.extend(batch['labels'])\n",
    "        # move data to device\n",
    "        batch = {k: v.to(predictor.args.device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = predictor.model(**batch)\n",
    "        logits = outputs[1]\n",
    "        llr.extend((logits[:, 1] - logits[:, 0]).tolist())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'prediction': predictions, 'llr': llr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep predictions, join back to val_df\n",
    "val_df = pd.concat((val_df.reset_index(), preds_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the change points as follows\n",
    "# filter for utterances where the prediction is 1\n",
    "# get the midpoint of that utterance (or start or end?) as the timestamp\n",
    "change_point_df = val_df[val_df['prediction'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'file_id', 'split', 'anno_start', 'anno_end', 'url',\n",
       "       'status_in_corpora', 'data_type', 'release', 'processed', 'start',\n",
       "       'end', 'text', 'avg_logprob', 'no_speech_prob', 'audio_files',\n",
       "       'video_frames', 'timestamp', 'impact_scalar', 'comment', 'annotator',\n",
       "       'labels', 'input_ids', 'social_orientation_preds', 'social_orientation',\n",
       "       'prediction', 'llr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring code is expecting, the following format for predictions\n",
    "#       [\n",
    "#           {'file_id': 'M010015BY', 'timestamp': 1160.2, 'type': 'audio', 'llr': 1.5},\n",
    "#           {'file_id': 'M010015BY', 'timestamp': 1287.67, 'type': 'audio', 'llr': 1.5},\n",
    "#           {'file_id': 'M010029SP', 'timestamp': 288, 'type': 'text', 'llr': 1.5},\n",
    "#           {'file_id': 'M010005QD', 'timestamp': 90.2, 'llr': 1.5, 'type': 'video'},\n",
    "#           {'file_id': 'M010019QD', 'timestamp': 190, 'llr': 1.5, 'type': 'text'}\n",
    "#       ]\n",
    "change_point_df = change_point_df[['file_id', 'start', 'data_type', 'llr']].rename(columns={'start': 'timestamp', 'data_type': 'type'})\n",
    "hyps = change_point_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reference data with the following expected format\n",
    "#       [\n",
    "#           {'file_id': 'M010015BY', 'timestamp': 1160.2, 'type': 'audio', 'impact_scalar': 4},\n",
    "#           {'file_id': 'M010015BY', 'timestamp': 1287.6, 'type': 'audio', 'impact_scalar': 2},\n",
    "#           {'file_id': 'M010029SP', 'timestamp': 288.0, 'type': 'text', 'impact_scalar': 1},\n",
    "#           {'file_id': 'M010005QD', 'timestamp': 90.2, 'type': 'video', 'impact_scalar': 5},\n",
    "#           {'file_id': 'M010019QD', 'timestamp': 90, 'type': 'text', 'impact_scalar': 5}\n",
    "#       ]\n",
    "\n",
    "# load the reference data to be sure we've got all the data points\n",
    "data = load_ldc_data(False, True)\n",
    "val_data = {k: v for k, v in data.items() if data[k]['split'] == 'val'}\n",
    "refs = []\n",
    "for file_id in val_data:\n",
    "    for changepoint in val_data[file_id]['changepoints']:\n",
    "        refs.append({'file_id': file_id, 'timestamp': changepoint['timestamp'], 'type': val_data[file_id]['data_type'], 'impact_scalar': changepoint['impact_scalar']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time is 193.59772488474846\n"
     ]
    }
   ],
   "source": [
    "# filtering options {'none', 'highest', 'lowest', 'most_similar'}\n",
    "start = time.perf_counter()\n",
    "results = {}\n",
    "mean_scores = []\n",
    "for filtering in ['none', 'highest', 'lowest', 'most_similar']:\n",
    "    print(f'filtering with {filtering}')\n",
    "    results[filtering] = calculate_average_precision(refs,\n",
    "                                hyps,\n",
    "                                text_char_threshold=100,\n",
    "                                time_sec_threshold=10,\n",
    "                                filtering=filtering)\n",
    "    mean_scores.append((filtering, statistics.harmonic_mean(results[filtering].values())))\n",
    "end = time.perf_counter()\n",
    "print(f'elapsed time is {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the filtering procedure that gives the best average precision across modalities with a greater\n",
    "mean_scores = []\n",
    "for filtering_approach in results:\n",
    "    mean_scores.append((filtering_approach, statistics.harmonic_mean(results[filtering_approach].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best filtering approach\n",
    "filtering = max(mean_scores, key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump predictions to csv with the following columns\n",
    "# file_id, timestamp, llr\n",
    "# filter predictions according to the best procedure\n",
    "final_preds = filter_system_preds(hyps, text_char_threshold=100, time_sec_threshold=10, filtering=filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds_df = pd.DataFrame.from_dict(final_preds)[['file_id', 'timestamp', 'llr']]\n",
    "os.path.basename(predictor.model_dir)\n",
    "# save to predictions folder with the same name as the model_dir\n",
    "preds_dir = f'/mnt/swordfish-pool2/ccu/predictions/{os.path.basename(predictor.model_dir)}'\n",
    "os.makedirs(preds_dir, exist_ok=True)\n",
    "final_preds_df.to_csv(os.path.join(preds_dir, 'val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Change Point', 0: 'No Change Point'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.args.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "No Change Point       0.91      1.00      0.95     56499\n",
      "   Change Point       0.00      0.00      0.00      5433\n",
      "\n",
      "       accuracy                           0.91     61932\n",
      "      macro avg       0.46      0.50      0.48     61932\n",
      "   weighted avg       0.83      0.91      0.87     61932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, predictions, target_names=['No Change Point', 'Change Point'], labels=[0, 1], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
