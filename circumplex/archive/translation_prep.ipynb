{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare ASR Transcription Queue from Chinese to English\n",
    "# WARNING! Running this script may overwrite the translation directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import json\n",
    "import queue\n",
    "\n",
    "from googletrans import Translator\n",
    "import googletrans\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_dirs = [os.path.join(home_dir, 'Documents/datasets/charm/transformed/R2/ldc-r2-batch1-tom-n79'),\n",
    "os.path.join(home_dir, 'Documents/datasets/charm/transformed/R1/audio_processed'),\n",
    "os.path.join(home_dir, 'Documents/datasets/charm/transformed/R1/video_processed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of all filepaths and file_ids\n",
    "asr_files = []\n",
    "file_ids = []\n",
    "files_by_dir = defaultdict(list)\n",
    "dir_by_file = {}\n",
    "for dir_ in asr_dirs:\n",
    "    for f in os.listdir(dir_):\n",
    "        if f.endswith('.json'):\n",
    "            filepath = os.path.join(dir_, f)\n",
    "            asr_files.append(filepath)\n",
    "            file_ids.append(f.split('_')[0])\n",
    "            group = os.path.join(*filepath.split(os.sep)[-3:-1])\n",
    "            files_by_dir[group].append(filepath)\n",
    "            dir_by_file[os.path.split(filepath)[-1]] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json files\n",
    "raw_data = {}\n",
    "data_dfs = {}\n",
    "for f in asr_files:\n",
    "    filename = os.path.split(f)[-1]\n",
    "    with open(f, 'r') as fh:\n",
    "        raw_data[filename] = json.load(fh)\n",
    "        if 'asr_turn_lvl' in raw_data[filename]:\n",
    "            data_dfs[filename] = pd.DataFrame(raw_data[filename]['asr_turn_lvl'])\n",
    "        else:\n",
    "            data_dfs[filename] = pd.DataFrame(raw_data[filename]['asr_preprocessed_turn_lvl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDC annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://drive.google.com/drive/folders/1aL7bcLWQmUskR3dmj3K1jdXQsb_nIcv2\n",
    "anno_dir = os.path.join(home_dir, 'Documents/datasets/charm/raw/LDC2022E18_CCU_TA1_Mandarin_Chinese_Development_Annotation_V1.0/data')\n",
    "anno_files = [os.path.join(anno_dir, x) for x in os.listdir(anno_dir) if x not in ['.DS_Store']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dfs = {}\n",
    "for f in anno_files:\n",
    "    filename = os.path.split(f)[-1]\n",
    "    anno_dfs[filename] = pd.read_csv(f, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_files = {}\n",
    "anno_files_list = set()\n",
    "for f in anno_dfs:\n",
    "    temp_files = anno_dfs[f]['file_id'].unique()\n",
    "    anno_files[f] = temp_files\n",
    "    anno_files_list = anno_files_list.union(set(temp_files))\n",
    "anno_files_list = sorted(list(anno_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which files do we have transcriptions for?\n",
    "ldc_intersection = set(anno_files_list).intersection(set(file_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ldc_intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify number of translations and characters per translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calls_per_trans = []\n",
    "num_chars_per_utter = []\n",
    "for f in ldc_intersection:\n",
    "    temp_df = data_dfs[f'{f}_processed_results.json']\n",
    "    num_calls_per_trans.append(len(temp_df))\n",
    "    chars_per_utter = temp_df['transcript'].apply(lambda x: len(x)).values.tolist()\n",
    "    num_chars_per_utter.extend(chars_per_utter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>API Calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>220.885417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>185.040187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>162.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>279.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1104.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         API Calls\n",
       "count    96.000000\n",
       "mean    220.885417\n",
       "std     185.040187\n",
       "min      28.000000\n",
       "25%     100.750000\n",
       "50%     162.500000\n",
       "75%     279.250000\n",
       "max    1104.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats on API calls, including number of API calls (count), average and max number of utterances (mean, max) \n",
    "pd.DataFrame(num_calls_per_trans, columns=['API Calls']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.635416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>49.113722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2244.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Utterances\n",
       "count  21205.000000\n",
       "mean      27.635416\n",
       "std       49.113722\n",
       "min        1.000000\n",
       "25%       11.000000\n",
       "50%       18.000000\n",
       "75%       30.000000\n",
       "max     2244.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats on utterances, including number of utterances (count), average and max number of characters (mean, max) \n",
    "pd.DataFrame(num_chars_per_utter, columns=['Utterances']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586009"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of chars translated\n",
    "sum(num_chars_per_utter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a process for keeping track of successes/failures\n",
    "- we can have partial success on the list of files\n",
    "- each file can have partial success on the utterances\n",
    "- work queue should be a list of files\n",
    "- where each element in the queue is a DF containing all utterances and translated column\n",
    "- can then reprocess all results idempotently, by checking if the translated column is null or not\n",
    "- using a queue also sets us up to use threads in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(home_dir, 'Documents/datasets/charm/transformed/translations')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write initial queue to output directory, then all future jobs will read from this directory to push toward completion\n",
    "# warning, only do this once, otherwise work will be overwritten\n",
    "overwrite_cache = False\n",
    "if overwrite_cache:\n",
    "\n",
    "    # add in placeholder transcript_en column into all DFs\n",
    "    initial_queue = {}\n",
    "    for f in data_dfs:\n",
    "        # only queue up files that we have labels for\n",
    "        if f.split('_')[0] not in ldc_intersection:\n",
    "            continue\n",
    "        data_dfs[f]['transcript_en'] = np.NaN\n",
    "        asr_turn_lvl = data_dfs[f].to_dict(orient='records')\n",
    "        # copy the data over\n",
    "        initial_queue[f] = {**raw_data[f]} \n",
    "        # standardize this key\n",
    "        if 'asr_turn_lvl' in initial_queue[f]:\n",
    "            initial_queue[f]['asr_turn_lvl'] = asr_turn_lvl\n",
    "        else:\n",
    "            # delete 'asr_preprocessed_turn_lvl' and make it 'asr_turn_lvl'\n",
    "            initial_queue[f].pop('asr_preprocessed_turn_lvl')\n",
    "            initial_queue[f]['asr_turn_lvl'] = asr_turn_lvl\n",
    "\n",
    "        # write this initial queue to disk\n",
    "        file_id = f.split('_')[0] + '.json' # just use file_id.json as the filename\n",
    "        filepath = os.path.join(output_dir, file_id)\n",
    "        with open(filepath, 'w', encoding='utf-8') as fh:\n",
    "            json.dump(initial_queue[f], fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload saved data and verify correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite_cache:\n",
    "    queue_check = {}\n",
    "    translation_files = []\n",
    "    for x in os.listdir(output_dir):\n",
    "        if x.endswith('.json'):\n",
    "            filepath = os.path.join(output_dir, x)\n",
    "            translation_files.append(filepath)\n",
    "            file_id = x.split('.')[0]\n",
    "            with open(filepath, 'r', encoding='utf-8') as fp:\n",
    "                queue_check[file_id] = json.load(fp)\n",
    "\n",
    "            # check that the dict is equivalent to the original dict\n",
    "            # this got nightmarishly complex due to the presence of the \"transcript_en\" key\n",
    "            raw_data_key = f'{file_id}_processed_results.json'\n",
    "            for key in raw_data[raw_data_key]:\n",
    "                if key == 'asr_preprocessed_turn_lvl':\n",
    "                    for idx, element in enumerate(raw_data[raw_data_key][key]):\n",
    "                        for subkey in element:\n",
    "                            assert element[subkey] == queue_check[file_id]['asr_turn_lvl'][idx][subkey]\n",
    "                elif key == 'asr_turn_lvl':\n",
    "                    for idx, element in enumerate(raw_data[raw_data_key][key]):\n",
    "                        for subkey in element:\n",
    "                            assert element[subkey] == queue_check[file_id][key][idx][subkey]\n",
    "                else:\n",
    "                    assert raw_data[raw_data_key][key] == queue_check[file_id][key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
