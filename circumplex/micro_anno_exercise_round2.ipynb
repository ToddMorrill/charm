{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f30a74-9c7c-4056-a861-75d038897d94",
   "metadata": {},
   "source": [
    "# Identify known changepoints for annotation\n",
    "- identify 30 videos and define 1 span containing a change point and 1 span containing a known negative segment\n",
    "\n",
    "TODO:\n",
    "- download updates\n",
    "- update metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a70a71ec-6b6e-4071-a437-793002505bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "import random\n",
    "import hashlib\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from charm.eval.eval import mapping, categorize_pairs, precision, recall\n",
    "from charm.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec14abe-67a4-4ddd-a3e0-ce0434326074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "meta_df = pd.read_csv('/home/iron-man/Documents/data/charm/transformed/metadata.csv')\n",
    "\n",
    "# load annotations\n",
    "anno_dict = utils.load_ldc_annotations('/home/iron-man/Documents/data/charm/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b141fba2-b029-4e9b-9698-b285b78baa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point_df = meta_df[meta_df['changepoint_count'] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d045bbe5-f9bf-4add-96a8-3c2ba85b7743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R3           1197\n",
       "Mini-Eval    1088\n",
       "R2           1051\n",
       "R1            413\n",
       "Name: release, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_point_df['release'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ebbb23-401b-4efe-8d0f-6e449ed17ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point_df = change_point_df[(change_point_df['release'] == 'Mini-Eval') & (change_point_df['modality'] == 'video')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48f8d9d-5e6a-429a-aec0-c06ca4f2eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point_df = change_point_df.sample(n=30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62454849-30f3-42d4-b1d3-ada59210e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_eval_df = anno_dict['Mini-Eval-Annotations']['anno_dfs']['changepoint.tab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449824a7-b44a-4666-a8e0-2362dcdc0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in meta_df\n",
    "mini_eval_df = mini_eval_df.merge(meta_df, how='left', left_on='file_id', right_on='file_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41dcc2e-19c3-4974-9b24-03df6cc0fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_eval_df = mini_eval_df[mini_eval_df['modality'] == 'video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd099d9e-c14a-423a-99ed-707216e1e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample one change point from each file_id, then sample 30 changepoints\n",
    "sample_df = mini_eval_df.groupby('file_id').sample(n=1, random_state=42).sample(n=30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82934765-6eca-4f9d-ba99-86a0aadc3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all file_ids from sample_df, and then remove 40 second windows (+/- 20 secs) around each change point\n",
    "# among remaining intervals, chunk into 40 second intervals\n",
    "# randomly select 1\n",
    "sample_annos_df = mini_eval_df[mini_eval_df['file_id'].isin(sample_df['file_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81995395-9742-4769-98d3-1961b640b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226133/1146366723.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_annos_df['timestamp_start_raw'] = sample_annos_df['timestamp'].apply(lambda x: max(float(x) - 20, 0))\n",
      "/tmp/ipykernel_226133/1146366723.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_annos_df['timestamp_end_raw'] = sample_annos_df['timestamp'].apply(lambda x: float(x) + 20)\n"
     ]
    }
   ],
   "source": [
    "sample_annos_df['timestamp_start_raw'] = sample_annos_df['timestamp'].apply(lambda x: max(float(x) - 20, 0)) \n",
    "sample_annos_df['timestamp_end_raw'] = sample_annos_df['timestamp'].apply(lambda x: float(x) + 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9740aa1-3cb7-4f2c-95a7-a62537a2dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226133/1338084517.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_annos_df['timestamp_start'] =  sample_annos_df[['timestamp_start_raw', 'start']].max(axis=1)\n",
      "/tmp/ipykernel_226133/1338084517.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_annos_df['timestamp_end'] =  sample_annos_df[['timestamp_end_raw', 'end']].min(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# ensure we start and end within the LDC annotated region\n",
    "sample_annos_df['timestamp_start'] =  sample_annos_df[['timestamp_start_raw', 'start']].max(axis=1)\n",
    "sample_annos_df['timestamp_end'] =  sample_annos_df[['timestamp_end_raw', 'end']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c2bc47b-dae3-4ddb-937a-1e71c49bec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(group_df):\n",
    "    # create tuples out of start and end interval\n",
    "    start = group_df['timestamp_start'].values.tolist()\n",
    "    end = group_df['timestamp_end'].values.tolist()\n",
    "    timestamps = group_df['timestamp'].values.tolist()\n",
    "    impact_scalars = group_df['impact_scalar'].values.tolist()\n",
    "    start_end = list(zip(start, end))\n",
    "    all_vals = list(zip(start_end, timestamps, impact_scalars))\n",
    "    # sort by timestamp\n",
    "    sorted_vals = sorted(all_vals, key=lambda x: x[1])\n",
    "    # now unzip to separate columns\n",
    "    return pd.Series(zip(*sorted_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "753f5d06-f6ad-45de-81f4-a0256e0ca9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotated segments for each file\n",
    "interval_df = sample_annos_df.groupby(['file_id'], as_index=True).apply(create_segments).rename(columns={0: 'intervals', 1: 'timestamp', 2: 'impact_scalar'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "40af8e1b-d874-46ef-ba6c-3de87ebc96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intervals column contains changepoint labeled intervals\n",
    "# # sort by starting times\n",
    "# interval_df['intervals'] = interval_df['intervals'].apply(lambda x: sorted(x, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1889f061-4f5e-4b2f-938d-d15f6e651d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if intervals overlap or if gap between intervals is less than 40 seconds, merge\n",
    "def merge_intervals(intervals):\n",
    "    final_intervals = [intervals[0]]\n",
    "    for start, end in intervals[1:]:\n",
    "        # start of next interval isn't more than 40 seconds after the end of the previous interval, merge\n",
    "        if start < final_intervals[-1][1] + 40:\n",
    "            new_start = final_intervals[-1][0]\n",
    "            new_end = end\n",
    "            final_intervals[-1] = (new_start, new_end)\n",
    "        else:\n",
    "            final_intervals.append((start, end))\n",
    "    return final_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b8175b5f-7371-4593-b474-62a6da053cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df['merged_intervals'] = interval_df['intervals'].apply(lambda x: merge_intervals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "014c10a5-0bf0-4c5e-91f4-edf3219bb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in start/end times to extract 40 second intervals without annotations\n",
    "interval_df = interval_df.reset_index()\n",
    "interval_df = interval_df.merge(sample_annos_df[['file_id', 'start', 'end']].drop_duplicates(), how='left', on='file_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4f47c12a-6171-4413-bf49-5de83540df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df['start_end'] = list(zip(interval_df['start'], interval_df['end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "95814ef5-0ae9-4b42-81df-56d037ea5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(interval, chunk_size):\n",
    "    \"\"\"Create chunk_size blocks of time based on interval=(start, end).\"\"\"\n",
    "    chunks = []\n",
    "    for i in np.arange(interval[0], interval[1], chunk_size):\n",
    "        # drop last chunk\n",
    "        if i + chunk_size > interval[1]:\n",
    "            continue\n",
    "        chunks.append((i, i + chunk_size))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4aebafab-701d-4ec9-87e6-f917f5489d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_intervals(row):\n",
    "    \"\"\"Identify all regions not tagged as a change point and create 40 second chunks.\"\"\"\n",
    "    free_intervals = []\n",
    "    start_time = row['start_end'][0]\n",
    "    for start, end in row['merged_intervals']:\n",
    "        if start - start_time > 40:\n",
    "            free_intervals.extend(create_chunks((start_time, start), 40))\n",
    "        start_time = end\n",
    "\n",
    "\n",
    "    # add any time at the end\n",
    "    end = row['merged_intervals'][-1][1]\n",
    "    if row['start_end'][1] - end > 40:\n",
    "        free_intervals.extend(create_chunks((end, row['start_end'][1]), 40))\n",
    "    return free_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9620b2e7-4365-45ce-aaef-7bebc608b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df['free_intervals'] = interval_df.apply(lambda x: get_free_intervals(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf456f4-9ccf-4dbe-a91f-bdaaa492558d",
   "metadata": {},
   "source": [
    "### TODO: Select one labeled interval and one unlabeled interval\n",
    "Determine if we need to tie back to a known change point for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "61c61327-4644-42a6-997c-06f4d574f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "interval_df['chosen_interval'] = interval_df['intervals'].apply(lambda x: random.randint(0, len(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fe910c0f-c67c-4581-87e3-bbf5f29a5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain timestamp and impact_scalar for provenance\n",
    "interval_df['change_point'] = interval_df.apply(lambda x: x['intervals'][x['chosen_interval']], axis=1)\n",
    "interval_df['timestamp_selected'] = interval_df.apply(lambda x: x['timestamp'][x['chosen_interval']], axis=1)\n",
    "interval_df['impact_scalar_selected'] = interval_df.apply(lambda x: x['impact_scalar'][x['chosen_interval']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7ffd32be-d778-43ea-b5a6-95003029bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose a free interval as a non change point\n",
    "interval_df['non_change_point'] = interval_df['free_intervals'].apply(lambda x: random.choice(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e1420db6-b217-480a-8830-eceebfb6bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that change points don't overlap\n",
    "change_after_non = interval_df['change_point'].apply(lambda x: x[0]) >= interval_df['non_change_point'].apply(lambda x: x[1])\n",
    "change_before_non = interval_df['change_point'].apply(lambda x: x[1]) <= interval_df['non_change_point'].apply(lambda x: x[0])\n",
    "assert len(interval_df[~(change_after_non | change_before_non)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2bf1cb55-18dd-4c28-b3af-026457da4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = interval_df[['file_id', 'change_point', 'non_change_point', 'timestamp_selected', 'impact_scalar_selected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8d0edee7-5804-4626-a459-907749c10b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>change_point</th>\n",
       "      <th>non_change_point</th>\n",
       "      <th>timestamp_selected</th>\n",
       "      <th>impact_scalar_selected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M01003JVY</td>\n",
       "      <td>(147.0, 187.0)</td>\n",
       "      <td>(74.3, 114.3)</td>\n",
       "      <td>167.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M01003L5X</td>\n",
       "      <td>(292.0, 332.0)</td>\n",
       "      <td>(372.0, 412.0)</td>\n",
       "      <td>312.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01003LV3</td>\n",
       "      <td>(291.0, 331.0)</td>\n",
       "      <td>(226.0, 266.0)</td>\n",
       "      <td>311.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M01003MK7</td>\n",
       "      <td>(524.0, 564.0)</td>\n",
       "      <td>(383.2, 423.2)</td>\n",
       "      <td>544.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M01003N2J</td>\n",
       "      <td>(453.0, 493.0)</td>\n",
       "      <td>(283.6, 323.6)</td>\n",
       "      <td>473.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_id    change_point non_change_point  timestamp_selected  \\\n",
       "0  M01003JVY  (147.0, 187.0)    (74.3, 114.3)               167.0   \n",
       "1  M01003L5X  (292.0, 332.0)   (372.0, 412.0)               312.0   \n",
       "2  M01003LV3  (291.0, 331.0)   (226.0, 266.0)               311.0   \n",
       "3  M01003MK7  (524.0, 564.0)   (383.2, 423.2)               544.0   \n",
       "4  M01003N2J  (453.0, 493.0)   (283.6, 323.6)               473.0   \n",
       "\n",
       "   impact_scalar_selected  \n",
       "0                       3  \n",
       "1                       4  \n",
       "2                       2  \n",
       "3                       4  \n",
       "4                       4  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d5fd1600-0c9c-41f4-aba5-8de048d9c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = pd.melt(interval_df, id_vars=['file_id', 'timestamp_selected', 'impact_scalar_selected'], value_vars=['change_point', 'non_change_point'], value_name='interval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b3c030bb-3a9b-4b37-abb5-405d9138e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df['interval_start'] = interval_df['interval'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6e2dfc05-c7c0-4c7a-8c20-ee0c66e8cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = interval_df.sort_values(by=['file_id', 'interval_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "751db692-ee24-45c0-b77e-1db25d906d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df.loc[interval_df['variable'] == 'non_change_point', ['timestamp_selected', 'impact_scalar_selected']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d35ec6e5-0b2a-489a-8777-e621423388e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d3303039-33b3-4adb-a342-92855f757e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "interval_df.drop(columns=['interval_start'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59590cb5-dda3-4aff-8db3-dfec0ef808ae",
   "metadata": {},
   "source": [
    "### Create a list of these file_ids, download them, push to Google Drive, and get links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "16fb929e-9976-426f-b2f5-02a60fcdf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that all videos are not urls\n",
    "assert (meta_df[meta_df['file_uid'].isin(interval_df['file_id'].unique())]['url'] != 'na').sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "28fb36dc-bab2-4a67-8644-6c85c26e4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push these files to Google Drive\n",
    "from charm.data.gdrive.upload import create_folder, upload_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a60c10a5-dff0-4354-8af9-9f2e5ff48559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a folder within the Circumplex Theory folder\n",
    "# parent_dir = ['1w5L4T9LN0imrMhuTSXtPKODSzSKEHE2C']\n",
    "# folder_name = 'Annotation Videos'\n",
    "# dir_id = create_folder(folder_name, parents=parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2b169d5-9d05-4176-9818-31b58fe4b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_id = '1ts2M2iVGrYYNZBjTTufyWkCGjLcoaBTv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22f0113f-db26-48d8-9cca-020cd824043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mini-Eval    30\n",
       "Name: release, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload files from mini-eval folder\n",
    "meta_df[meta_df['file_uid'].isin(interval_df['file_id'].unique())]['release'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "10064bfb-6187-4422-9e99-69fe7e777714",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_eval_dir = '/home/iron-man/Documents/data/charm/raw/LDC2022E22_CCU_TA1_Mandarin_Chinese_Mini_Evaluation_Source_Data/data/video'\n",
    "# save back to mini_eval_dir and remove the .ldcc formatted file to save disk space\n",
    "# TODO: remove ldcc formatted file\n",
    "# TODO: loaders should first try to load .mp4 and will remove .ldcc headers if not found\n",
    "\n",
    "# loop over files and strip LDC header\n",
    "for file_id in interval_df['file_id']:\n",
    "    filename = f'{file_id}.mp4.ldcc'\n",
    "    file_path = os.path.join(mini_eval_dir, filename)\n",
    "    out_filepath = utils.strip_ldc_header(file_path, mini_eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0bf611f0-43bc-4849-a333-f120fe6488fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M01003JVY av1\n",
      "File: M01003JVY.mp4 had av1 codec. Converting..\n",
      "M01003L5X hevc\n",
      "File: M01003L5X.mp4 had hevc codec. Converting..\n",
      "M01003LV3 h264\n",
      "M01003MK7 av1\n",
      "File: M01003MK7.mp4 had av1 codec. Converting..\n",
      "M01003N2J av1\n",
      "File: M01003N2J.mp4 had av1 codec. Converting..\n",
      "M01003N5G h264\n",
      "M01003N7T h264\n",
      "M01003N9I av1\n",
      "File: M01003N9I.mp4 had av1 codec. Converting..\n",
      "M01003OZ6 h264\n",
      "M01003P4V av1\n",
      "File: M01003P4V.mp4 had av1 codec. Converting..\n",
      "M01003PP6 av1\n",
      "File: M01003PP6.mp4 had av1 codec. Converting..\n",
      "M01003Q62 av1\n",
      "File: M01003Q62.mp4 had av1 codec. Converting..\n",
      "M01003QIW h264\n",
      "M01003QOD h264\n",
      "M01003R7U h264\n",
      "M01003TO9 h264\n",
      "M01003UIN h264\n",
      "M01003VU6 av1\n",
      "File: M01003VU6.mp4 had av1 codec. Converting..\n",
      "M01003WSU h264\n",
      "M01003XPK h264\n",
      "M01003YUC h264\n",
      "M01003ZFK h264\n",
      "M01004D5B h264\n",
      "M01004GB3 h264\n",
      "M01004I6Q h264\n",
      "M01004KDW h264\n",
      "M01004KEK h264\n",
      "M01004NKE h264\n",
      "M01004V1Z h264\n",
      "M01004W7Z h264\n"
     ]
    }
   ],
   "source": [
    "# this will take some time\n",
    "file_paths = []\n",
    "for file_id in interval_df['file_id'].unique():\n",
    "    filename = f'{file_id}.mp4'\n",
    "    file_path = os.path.join(mini_eval_dir, filename)\n",
    "    file_paths.append(file_path)\n",
    "    probe_command = ['ffprobe', \n",
    "               '-show_format', \n",
    "               '-show_streams', '-loglevel',\n",
    "               'quiet',\n",
    "               '-print_format',\n",
    "               'json',\n",
    "               file_path]\n",
    "\n",
    "    out = subprocess.check_output(probe_command)\n",
    "    file_info = json.loads(out.decode())\n",
    "    codec_name = None\n",
    "    for stream in file_info['streams']:\n",
    "        if stream['codec_type'] == 'video':\n",
    "            codec_name = stream['codec_name']\n",
    "    \n",
    "    print(file_id, codec_name)\n",
    "    \n",
    "    # if problematic codec, convert using ffmpeg\n",
    "    if codec_name in ['av1', 'hevc']:\n",
    "        print(f'File: {filename} had {codec_name} codec. Converting..')\n",
    "        # load raw data\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        ffmpeg_command = ['ffmpeg',\n",
    "                  '-y',\n",
    "                  '-i',\n",
    "                  '-',\n",
    "                  '-loglevel',\n",
    "                  'quiet',\n",
    "                  '-vcodec',\n",
    "                  'libx264',\n",
    "                  file_path]\n",
    "        proc = subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        out, err = proc.communicate(input=raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6081f3b5-d563-458c-8191-7e0ecc16685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# piped input\n",
    "# probe_command = ['ffprobe', \n",
    "#            '-show_format', \n",
    "#            '-show_streams', '-loglevel',\n",
    "#            'quiet',\n",
    "#            '-print_format',\n",
    "#            'json',\n",
    "#            '-']\n",
    "\n",
    "# proc = subprocess.Popen(probe_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "# out, err = proc.communicate(input=complete_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "60adca2e-507f-41b7-8941-0e24e2fb0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1ZTbIwiA2a3IxPl1onMmmd-a7XQFo8Vs0/view?usp=share_link\n",
    "# f'https://drive.google.com/open?id={id_}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a358c466-d487-48f4-9d7c-b063fd13576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ID: 1bzGkh4qEDNjiemuZcPU26hJb76iczBxc\n",
      "File ID: 1EF0flwbdQacRr_GjIY2Dn6Tm-YpmqO5c\n",
      "File ID: 10jd5nuY-KAc9u1dYHUgBkIRTcHlmYW4b\n",
      "File ID: 1Ogr0UDivdOxMyO_9hlgVuhTVTT4yF3Xo\n",
      "File ID: 1D8340xsBZMmOefciMgKsIUENC2OK7B3q\n",
      "File ID: 1VS37ePAocRjclQEF0WT2z9QGr62_QpdA\n",
      "File ID: 19OrkAR0Nxyb8uYS_PLm_P87Aw4eGkJkl\n",
      "File ID: 1x0T7Fm9kMRCzOLo7QBVZOoAsihGHqfQy\n",
      "File ID: 1N05E655E1_PuC0tugGIrQ8tlWFiInBes\n",
      "File ID: 1iQk13eSE6KQY8lj98gQm2jOU0zOzvkm6\n",
      "File ID: 11VkRAsf3_IyXtNXC8pK-3sm4sV1ZpwZc\n",
      "File ID: 15saY3puv35IwEpoL3n48s2HWPnZcBzMp\n",
      "File ID: 1VrZgdzo9YbvCCFkYPvSj0z7qZhftOput\n",
      "File ID: 1hg22NgE-lDdE5DGumU6BKflGGNURNe29\n",
      "File ID: 1NnsCcre8xQ9-B-9l6JlQ3F9Ihssw9gxN\n",
      "File ID: 1dTBCzb4Smu6Xlz7vLLDJi-MEpRcecbBI\n",
      "File ID: 1IOgnkwqTciWtI7u78LB3a3bjhn4wcZIZ\n",
      "File ID: 1YDjg0s_MDbryrkfimQfd4Rha7bAAAXY4\n",
      "File ID: 1ruiLL3nQQZk5R4IWKyNH1YVy1hGSCmU4\n",
      "File ID: 1k_xS_6GiKzTXjpviJ9l8FCvhaK3qPg-6\n",
      "File ID: 1cHECuyVko7-upcSE0GLycWwgUbdBvo1c\n",
      "File ID: 1K7aGvZaN4d6LiJLjnrQu_L6xEk7QwQYd\n",
      "File ID: 10Q9qaKlPFCtcN7mSBXNIz9kegskYx7AI\n",
      "File ID: 1S3zj2Ho9G4F-4ea9675qPk7yaurtwQfM\n",
      "File ID: 1CpT7m5oKChHnYsVaauVi2UURvlJwklvk\n",
      "File ID: 1cuwOQFQrlVFL7R8VDbkuNuk17F1rRbJJ\n",
      "File ID: 1AKwmTGLY0MmKNxzK-c-HyPyE4Tz_MD_9\n",
      "File ID: 198ivqJqzjZq2sfLDSASuQSIHtfEaRpln\n",
      "File ID: 1ehg6I_jFu6TxUtReh4m7jVTG4WwlqqFM\n",
      "File ID: 1S989igc7Et83L8lqcTIyW6od23p-ztCc\n"
     ]
    }
   ],
   "source": [
    "# store file_ids linked to gdrive ids\n",
    "# TODO: check if we've already uploaded\n",
    "uploads = []\n",
    "for file_path in file_paths:\n",
    "    file_id = os.path.basename(file_path).split('.')[0]\n",
    "    gdrive_file_id = upload_basic(file_path, parents=[dir_id])\n",
    "    uploads.append((file_id, gdrive_file_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "98046301-c7db-4876-a611-3c26e5f17ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_df = pd.DataFrame(uploads, columns=['file_id', 'gdrive_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f497c1d0-ec9d-4ec1-a94d-110726b610c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_df['url'] = gdrive_df['gdrive_id'].apply(lambda x: f'https://drive.google.com/open?id={x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2a1e7be2-ec4d-484c-b1bb-0ec5282538ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>gdrive_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M01003JVY</td>\n",
       "      <td>1bzGkh4qEDNjiemuZcPU26hJb76iczBxc</td>\n",
       "      <td>https://drive.google.com/open?id=1bzGkh4qEDNji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M01003L5X</td>\n",
       "      <td>1EF0flwbdQacRr_GjIY2Dn6Tm-YpmqO5c</td>\n",
       "      <td>https://drive.google.com/open?id=1EF0flwbdQacR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01003LV3</td>\n",
       "      <td>10jd5nuY-KAc9u1dYHUgBkIRTcHlmYW4b</td>\n",
       "      <td>https://drive.google.com/open?id=10jd5nuY-KAc9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M01003MK7</td>\n",
       "      <td>1Ogr0UDivdOxMyO_9hlgVuhTVTT4yF3Xo</td>\n",
       "      <td>https://drive.google.com/open?id=1Ogr0UDivdOxM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M01003N2J</td>\n",
       "      <td>1D8340xsBZMmOefciMgKsIUENC2OK7B3q</td>\n",
       "      <td>https://drive.google.com/open?id=1D8340xsBZMmO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_id                          gdrive_id  \\\n",
       "0  M01003JVY  1bzGkh4qEDNjiemuZcPU26hJb76iczBxc   \n",
       "1  M01003L5X  1EF0flwbdQacRr_GjIY2Dn6Tm-YpmqO5c   \n",
       "2  M01003LV3  10jd5nuY-KAc9u1dYHUgBkIRTcHlmYW4b   \n",
       "3  M01003MK7  1Ogr0UDivdOxMyO_9hlgVuhTVTT4yF3Xo   \n",
       "4  M01003N2J  1D8340xsBZMmOefciMgKsIUENC2OK7B3q   \n",
       "\n",
       "                                                 url  \n",
       "0  https://drive.google.com/open?id=1bzGkh4qEDNji...  \n",
       "1  https://drive.google.com/open?id=1EF0flwbdQacR...  \n",
       "2  https://drive.google.com/open?id=10jd5nuY-KAc9...  \n",
       "3  https://drive.google.com/open?id=1Ogr0UDivdOxM...  \n",
       "4  https://drive.google.com/open?id=1D8340xsBZMmO...  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdrive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6dcb80c2-c73f-4137-9efb-cd1d759896b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://drive.google.com/open?id=1bzGkh4qEDNjiemuZcPU26hJb76iczBxc'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdrive_df['url'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4095f0b3-098a-4c8e-9d82-c92d24567222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into interval df\n",
    "interval_df = interval_df.merge(gdrive_df, how='left', on='file_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "de415bf1-9124-4cb0-9975-554d3e38b776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_id', 'timestamp_selected', 'impact_scalar_selected', 'variable',\n",
       "       'interval', 'gdrive_id', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f8a4fa41-d94d-49b7-bd57-a2488edc1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {'file_id': 'File ID', 'interval': 'Interval', 'url': 'URL'}\n",
    "interval_df = interval_df.rename(columns=column_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8439dbb7-c370-4bf3-9f7d-c566ec01cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in Annotator, Speaker ID, Start Tag, End Tag, Speaker Descriptor, Annotator Notes\n",
    "new_cols = ['Annotator', 'Speaker ID', 'Start Tag', 'End Tag', 'Speaker Descriptor', 'Annotator Notes']\n",
    "interval_df = interval_df.reindex(columns=interval_df.columns.tolist() + new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5c17ba9e-8d1f-4e44-95d7-fb17c9c465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_col_order = ['Annotator', 'File ID', 'URL', 'Interval', 'Speaker ID', 'Start Tag', 'End Tag', 'Speaker Descriptor', 'Annotator Notes', 'timestamp_selected', 'impact_scalar_selected', 'variable', 'gdrive_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d7d80260-fd26-4aee-99af-722943b19f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = interval_df[final_col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b5cabea7-e636-459e-8571-18498aeee066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up formatting\n",
    "# convert start/stop seconds to minute:seconds\n",
    "def convert_seconds(start_end):\n",
    "    interval = ['(']\n",
    "    for idx, seconds in enumerate(start_end):\n",
    "        m, s = divmod(seconds, 60)\n",
    "        interval.append(f'{int(m)}:{s:02.0f}')\n",
    "        if idx == 0:\n",
    "            interval.append(', ')\n",
    "    interval.append(')')\n",
    "    return ''.join(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d7a362fb-0f4c-4c07-b025-a9125ec66d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df['Interval'] = interval_df['Interval'].apply(lambda x: convert_seconds(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0a0c708f-c954-4236-827c-9b897932a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a version containing all metadata\n",
    "labeled_filepath = '/home/iron-man/Documents/data/charm/transformed/annotations/circumplex_60_intervals_labeled.csv'\n",
    "interval_df.to_csv(labeled_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f23c263c-186e-4ab2-8f6e-7848c26593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a version containing only annotator data\n",
    "unlabeled_filepath = '/home/iron-man/Documents/data/charm/transformed/annotations/circumplex_60_intervals.csv'\n",
    "interval_df[['Annotator', 'File ID', 'URL', 'Interval', 'Speaker ID', 'Start Tag', 'End Tag', 'Speaker Descriptor', 'Annotator Notes']].to_csv(unlabeled_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acc127-bc07-4daa-86de-a423b76a25df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CHARM)",
   "language": "python",
   "name": "charm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
