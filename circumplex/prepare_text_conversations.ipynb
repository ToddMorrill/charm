{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d70db5-2545-4ee9-8162-a0743985da5d",
   "metadata": {},
   "source": [
    "# Prepare a sample from the BOLT text message corpus for Amazon Mechanical Turk labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3302a994-d1e0-4e29-b055-df1f997b932f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "import tiktoken\n",
    "\n",
    "from charm.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c8b889-b30b-4474-a390-7a9b4d6a2961",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load metadata and identify text conversations that have changepoint labels\n",
    "data_dir = '/home/iron-man/Documents/data/charm'\n",
    "meta_df = pd.read_csv(os.path.join(data_dir, 'transformed/metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7d6bc1-fe73-410a-8515-e638a32d16a4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "change_point_annotated = meta_df['changepoint_count'] > 0\n",
    "text_modality = meta_df['modality'] == 'text'\n",
    "text_anno_df = meta_df[change_point_annotated & text_modality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f203450f-d6c9-4c07-b60c-dbdd3b36c56f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_anno_df['release'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ec16bf-1231-43a6-8375-c2fdfcdfc2ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "r1 = 'LDC2022E11_CCU_TA1_Mandarin_Chinese_Development_Source_Data_R1'\n",
    "r1_text_dir = os.path.join(data_dir, f'raw/{r1}/data/text')\n",
    "ltf_dir = os.path.join(r1_text_dir, 'ltf')\n",
    "psm_dir = os.path.join(r1_text_dir, 'psm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ac2c8b-f8eb-4f3f-be37-aae492dc3ef5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# mini eval dir\n",
    "mini_eval = 'LDC2022E22_CCU_TA1_Mandarin_Chinese_Mini_Evaluation_Source_Data'\n",
    "mini_eval_text_dir = os.path.join(data_dir, f'raw/{mini_eval}/data/text')\n",
    "mini_eval_ltf_dir = os.path.join(mini_eval_text_dir, 'ltf')\n",
    "mini_eval_psm_dir = os.path.join(mini_eval_text_dir, 'psm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1dcd42-3b6a-439a-af5c-d9966277c5ab",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def load_conversation(ltf_file, psm_file):\n",
    "    # ltf_file = os.path.join(ltf_dir, ltf_file)\n",
    "    # psm_file = os.path.join(psm_dir, psm_file)\n",
    "    with open(ltf_file, 'r') as f:\n",
    "        ltf_content = f.read()\n",
    "    \n",
    "    try:\n",
    "        with open(psm_file, 'r') as f:\n",
    "            psm_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        psm_content = False\n",
    "    return ltf_content, psm_content\n",
    "\n",
    "def unpack_attributes(attribute):\n",
    "    attr_dict = {}\n",
    "    for attr in attribute:\n",
    "        attr_dict[attr['@name']] = attr['@value']\n",
    "    return attr_dict\n",
    "\n",
    "def merge_metadata(ltf_content, psm_content):\n",
    "    ltf = xmltodict.parse(ltf_content)\n",
    "    psm = xmltodict.parse(psm_content)\n",
    "    \n",
    "    # filter psm list to message attributes\n",
    "    message_attr = []\n",
    "    for d in psm['psm']['string']:\n",
    "        if d['@type'] == 'message':\n",
    "            message_attr.append(d)\n",
    "    \n",
    "    # unpack the message attributes\n",
    "    psm_df = pd.DataFrame(message_attr)\n",
    "    psm_attr_df = pd.DataFrame(psm_df['attribute'].apply(unpack_attributes).values.tolist())\n",
    "    \n",
    "    psm_df = pd.concat((psm_df.drop(columns=['attribute']), psm_attr_df), axis=1)\n",
    "    \n",
    "    ltf_df = pd.DataFrame(ltf['LCTL_TEXT']['DOC']['TEXT']['SEG'])\n",
    "    \n",
    "    # join ltf and psm on start_char\n",
    "    df = pd.merge(ltf_df, psm_df, left_on='@start_char', right_on='@begin_offset', how='left')\n",
    "    \n",
    "    # filter out messages where content length is 0 for a clean inner join\n",
    "    df = df[df['@char_length'] != '0'].reset_index(drop=True)\n",
    "    assert (len(df) == len(ltf_df))\n",
    "    # may still be missing attributes for each message\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3931d4c-9323-40df-bafb-1cbfa817e09f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ltf_files = [os.path.join(ltf_dir, f) for f in os.listdir(ltf_dir) if f != '.DS_Store']\n",
    "psm_files = [os.path.join(psm_dir, f) for f in os.listdir(psm_dir) if f != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83e7ea4-00db-4a13-8d96-b06131ebad1b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load mini-eval data too\n",
    "ltf_files += [os.path.join(mini_eval_ltf_dir, f) for f in os.listdir(mini_eval_ltf_dir) if f != '.DS_Store']\n",
    "psm_files += [os.path.join(mini_eval_psm_dir, f) for f in os.listdir(mini_eval_psm_dir) if f != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a58915-3b9c-4aca-a987-7364d4b8db61",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ltf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1aa947-b5db-4d46-beef-cb937e421b9e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(psm_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce5d1c6-16bc-404d-861d-717847cb377d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "errors = []\n",
    "for ltf_file in ltf_files:\n",
    "    psm_file = ltf_file.replace('ltf', 'psm')\n",
    "    ltf_content, psm_content = load_conversation(ltf_file, psm_file)\n",
    "    if psm_content == False:\n",
    "        errors.append((ltf_file, psm_file, 'PSM file not found'))\n",
    "        continue\n",
    "    df = merge_metadata(ltf_content, psm_content)\n",
    "    \n",
    "    if len(df[df['participant'].isna()]) > 0:\n",
    "        errors.append((ltf_file, psm_file, 'Attributes missing'))\n",
    "        continue\n",
    "    \n",
    "    # retain filename\n",
    "    df.insert(0, 'filename', ltf_file)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "522f4128-41a4-4cce-bb40-5d2713c3172a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame(errors, columns=['ltf_file', 'psm_file', 'error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085dd5ad-21cc-49a1-aca4-2d100b450537",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attributes missing    5\n",
       "Name: error, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95cc4e5e-0abe-41af-ba5e-9f097895c336",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1355"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0354dbe1-9400-4446-ae84-e1c2d841731f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6fcaf46-b640-4495-80d4-c5b12611a217",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['filename'] = df['filename'].apply(lambda x: os.path.split(x)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7db9a93-582f-4737-98c9-077c8817ca93",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1355"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of conversations\n",
    "df['filename'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c60139dc-d3d5-41f3-8549-4af3f5d7f301",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_file = text_anno_df.iloc[0]['file_uid'] + text_anno_df.iloc[0]['data_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b98e7e3-0c0a-4b01-b64e-5807677909cb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = df[df['filename'] == sample_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79e0d119-9e30-4e46-bb5b-af6584c27868",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df[['ORIGINAL_TEXT', 'time', 'participant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99b49bd-deff-4c6f-9cac-60b0fc7a21ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(columns={'ORIGINAL_TEXT':'Original Text', 'time': 'Time', 'participant': 'Participant'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76c03ad3-12a6-42f9-b2af-783db0a8c230",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "speaker_map = {}\n",
    "speakers = ['A', 'B']\n",
    "for idx, participant in enumerate(sample_df['Participant'].unique()):\n",
    "    speaker_map[participant] = speakers[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "568bb9c7-73cd-4d1e-beea-f6a2bdbbf24d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df['Participant'] = sample_df['Participant'].apply(lambda x: speaker_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b69e53c6-f1bd-4698-9138-d41bc27652df",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.reset_index(drop=True)\n",
    "sample_df.index.name = 'Utterance ID'\n",
    "sample_df = sample_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f3eb5ed-2dba-4e88-9f45-abe9ff9b65af",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def create_line(row):\n",
    "    # TODO: could optionally include the time\n",
    "    return f\"Speaker {row['Participant']} ({row['Utterance ID']}):  {row['Original Text']}\"\n",
    "\n",
    "sample_df['Complete Line'] = sample_df.apply(create_line, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "619f6e7b-aa70-49f4-8b70-9ffa5d042d69",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "conversation_string = '\\n'.join(sample_df['Complete Line'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fcbaabb-ca7a-458e-9799-799acb6f311e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load prompt to prepend:\n",
    "with open('prompt.txt', 'r') as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93b4a429-dc1a-48b1-a54d-bf24819205b0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# get 10 utterances at a clip to fit within ChatGPT\n",
    "prompts = []\n",
    "for i in range(0, len(sample_df), 10):\n",
    "    conversation_string = '\\n'.join(sample_df['Complete Line'].iloc[i: i+10].values.tolist())\n",
    "    prompts.append(prompt + '\\n\\n' + conversation_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e6c3177-aedf-476e-9ffe-2674456a0baf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load chat GPT output\n",
    "chat_gpt_lines = []\n",
    "with open('ChatGPT_output.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        else:\n",
    "            chat_gpt_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ad63c8e-35c5-427b-b1e1-f7a33b2099a8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "chat_gpt_labels = [x.split(': ')[1].split(' -')[0] for x in chat_gpt_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa94bb99-0d6c-44e6-bd85-8373967b5587",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "chat_gpt_explanations = [x.split(' - ')[1] for x in chat_gpt_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59af1cda-10b2-46af-9bc9-ef393d409594",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df['chat_gpt_labels'] = chat_gpt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb537d2e-fc77-4259-85db-edc82fbd687a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df['chat_gpt_explanations'] = chat_gpt_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c25c9636-9136-4496-9c98-d51e8cb1b5aa",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df['character_count'] = sample_df['Original Text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79d6bc1f-aca9-449b-b5d3-170954ec0b7e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df['character_count_cumsum'] = sample_df['character_count'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c0017ea-3349-4249-8f04-ce87148babf9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# get start/stop character intervals for each utterance\n",
    "# intervals will be [start, end)\n",
    "# start should be previous row character_count_cumsum\n",
    "# end should be start + character_count\n",
    "sample_df['start_character'] = sample_df['character_count_cumsum'].shift(1, fill_value=0.0)\n",
    "sample_df['end_character'] = sample_df['start_character'] + sample_df['character_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5693e6a6-c8a4-4e1a-90fb-c334024e0efc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_file_id = text_anno_df.iloc[0]['file_uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c28694a-92d5-4770-85d4-d4b60969586d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load annotations\n",
    "result = utils.load_ldc_annotations(os.path.join(data_dir, 'raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2585fd90-b9dd-4773-8878-70de99873992",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "changepoint_dfs = {}\n",
    "for anno in result:\n",
    "    changepoint_dfs[anno] = result[anno]['anno_dfs']['changepoint.tab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0468b54b-72f7-404b-8dd7-2948e2c2f43e",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "change_point_anno_df = pd.concat(changepoint_dfs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c211d847-6220-48b9-bd7e-ca0f6ed2ae66",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "change_point_anno_df['timestamp'] = change_point_anno_df['timestamp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58d7ec35-30bc-4091-a5b4-c77cf1847473",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "change_point_sample_anno_df = change_point_anno_df[change_point_anno_df['file_id'] == text_anno_df.iloc[0]['file_uid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da6aa595-7a59-4e54-b8f0-cf1148ff9d11",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = pd.merge_asof(sample_df, change_point_sample_anno_df[['timestamp', 'impact_scalar', 'comment']], left_on='start_character', right_on='timestamp', direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2e25079-7e87-4e13-a4a5-7333f36815ec",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# remove invalid matches\n",
    "# TODO: this doesn't solve for the issue of multiple changepoints in one utterance\n",
    "greater_equal = sample_df['start_character'] <= sample_df['timestamp']\n",
    "less = sample_df['timestamp'] < sample_df['end_character']\n",
    "sample_df.loc[~(greater_equal & less), ['timestamp', 'impact_scalar', 'comment']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "865bb71a-57fb-4b35-8013-87632b5d75a1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "save_df = sample_df[['Utterance ID', 'Participant', 'Time', 'Original Text', 'chat_gpt_labels', 'chat_gpt_explanations','timestamp', 'impact_scalar', 'comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fea612ef-e840-475e-9af4-cf3b8f9ca53a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# save_df.to_csv(f'{sample_file_id}_social_orientation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5186d-14d9-418e-99b9-00a1b4fdd133",
   "metadata": {},
   "source": [
    "### Save complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b53e2905-14f1-4eb4-8db1-e28d5b1d2ac1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "r1_transformed_dir = os.path.join(data_dir, f'transformed/{r1}/data/text')\n",
    "os.makedirs(r1_transformed_dir, exist_ok=True)\n",
    "df.to_csv(os.path.join(r1_transformed_dir, 'text.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c34331a-fbe5-4ccc-9a16-0ad90bf2634e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/iron-man/Documents/data/charm/transformed/LDC2022E11_CCU_TA1_Mandarin_Chinese_Development_Source_Data_R1/data/text'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_transformed_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074969de-0468-4ad9-a0f0-d445ca24a4f0",
   "metadata": {},
   "source": [
    "### Create and save a Circumplex version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24c5217f-42c4-4c8f-a64b-47f099e56240",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "circumplex_labels = ['Assured-Dominant', 'Gregarious-Extraverted', 'Warm-Agreeable', 'Unassuming-Ingenuous', 'Unassured-Submissive', 'Aloof-Introverted', 'Cold', 'Arrogant-Calculating']\n",
    "# generate random labels for now\n",
    "df['social_orientation'] = random.choices(circumplex_labels, k=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "267c308b-16ea-4de4-a5ef-102759a2a66b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(r1_transformed_dir, 'text_circumplex_random.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f25c2-070f-46a0-a0e1-0ce6afcab965",
   "metadata": {},
   "source": [
    "### Create GPT prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e75e093-5d83-4584-9c97-ae95063c667c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf85e52-310e-4f90-851a-38774101588f",
   "metadata": {},
   "source": [
    "### Data preparation plan\n",
    "1. Only annotate change point annotated conversations from R1 for now\n",
    "1. Convert participant IDs to speaker letters\n",
    "1. Merge in change point information\n",
    "1. Measure conversation length and split conversation into multiple chunks as needed\n",
    "1. Save to jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46d02f18-d369-4313-a9af-517183433853",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# label everything\n",
    "# text_anno_df = text_anno_df[text_anno_df['release'] == 'R1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27673fe8-8fb8-4090-9e92-9bfefb5c28b0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "text_file_ids = set(text_anno_df['file_uid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9abaf02c-6b39-4608-9d69-67f2c2f6b337",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# filter conversations df to these file_ids\n",
    "df['file_id'] = df['filename'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b57eac5-9c4d-49af-9755-ef782a06f834",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['file_id'].isin(text_file_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58e24880-813f-4571-ab12-08ce57d193ea",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "temp_df = df[df['filename'] == 'M01000GZR.ltf.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ca6520f-f525-4b60-aa1f-2e6b6d071516",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def id_speakers(group_df):\n",
    "    speaker_map = {}\n",
    "    for idx, participant in enumerate(group_df['participant'].unique()):\n",
    "        speaker_map[participant] = idx + 1\n",
    "\n",
    "    # apply speaker map to the participant column\n",
    "    group_df['participant'] = group_df['participant'].apply(lambda x: speaker_map[x])\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39a9d179-0dd6-4789-83b6-9e83c97440a3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# for each file_id, convert participants to numbers\n",
    "df = df.groupby('filename', group_keys=False).apply(id_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68493b27-6521-468d-bdac-ef48c423d1de",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['@begin_offset'] = df['@begin_offset'].astype(int)\n",
    "df['@char_length'] = df['@char_length'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39427e89-79d8-4b72-b3e7-ee8c7463c3fc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def merge_changepoints(group_df, change_point_anno_df):\n",
    "    # identify file_i\n",
    "    file_id = group_df['file_id'].iloc[0]\n",
    "    file_df = change_point_anno_df[change_point_anno_df['file_id'] == file_id].sort_values(by='timestamp')\n",
    "    # merge in changepoint data\n",
    "    merged_df = pd.merge_asof(group_df, file_df[['timestamp', 'impact_scalar', 'comment']], left_on='@begin_offset', right_on='timestamp', direction='nearest')\n",
    "    # remove invalid matches\n",
    "    # TODO: this doesn't solve for the issue of multiple changepoints in one utterance\n",
    "    greater_equal = merged_df['@begin_offset'] <= merged_df['timestamp']\n",
    "    less = merged_df['timestamp'] < (merged_df['@begin_offset'] + merged_df['@char_length'])\n",
    "    merged_df.loc[~(greater_equal & less), ['timestamp', 'impact_scalar', 'comment']] = np.nan\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1061123a-2e3e-48cb-9cff-178a44fabeb9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f9b310d-29e3-495b-bf1d-eb2a19664c22",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merge_changepoints_partial = partial(merge_changepoints, change_point_anno_df=change_point_anno_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a1aedbf-7275-4e96-a124-2fe3b6c20b80",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "change_point_anno_df['timestamp'] = change_point_anno_df['timestamp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db8d3152-bad5-4c86-baff-d9a9da850536",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "file_id = 'M01000GZR'\n",
    "file_df = change_point_anno_df[change_point_anno_df['file_id'] == file_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac8ec790-83dc-478b-a9dc-c09d57e5e4d8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.groupby('file_id', group_keys=False).apply(merge_changepoints_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "924b5cb8-e749-4aa8-8fd6-057402e726c8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['timestamp'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6306283-4a1e-465f-b756-561aab35f8f9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_point_anno_df['file_id'].isin(text_file_ids).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c2f0b43-8fe7-4b66-8bc8-dfd4ab97328d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# check that all change points available were used\n",
    "# assert df['timestamp'].notnull().sum() == change_point_anno_df['file_id'].isin(text_file_ids).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b02832ea-aa40-4ef4-87a3-6735497d8c1f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create utterance ID for each file\n",
    "def create_utterance_id(group_df):\n",
    "    group_df['Utterance ID'] = range(1, len(group_df)+1)\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ccc0423d-c602-45e6-b116-5505c631b356",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.groupby('file_id', group_keys=False).apply(create_utterance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef432f5a-ef50-48ae-b92e-74cf31555c7f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create conversation turn\n",
    "def create_line(row):\n",
    "    # TODO: could optionally include the time\n",
    "    return f\"Speaker {row['participant']} ({row['Utterance ID']}):  {row['ORIGINAL_TEXT']}\"\n",
    "\n",
    "df['Complete Line'] = df.apply(create_line, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01d2c2fd-d225-45f9-98e7-c88161510000",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-0301')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "954aeb21-73e7-494d-8327-28deae2c5caa",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['Complete Line Length'] =  df['Complete Line'].apply(lambda x: len(encoding.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19d1e0a3-51f1-44b7-931f-ad0f0493fd2d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def group_cumsum(group_df):\n",
    "    group_df['line_len_cumsum'] = group_df['Complete Line Length'].cumsum()\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fab9459-18db-4f08-a55e-27e5cfb29241",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.groupby('file_id', group_keys=False).apply(group_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e39b2d6e-1646-46a2-b023-ea656eef15b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# measure prompt length and split as needed\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9bc88a5-d0a2-4c6f-945d-49d26b9934c3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load prompt\n",
    "with open('prompt.txt', 'r') as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38b4e5e6-475f-4e54-94a1-09b23f57af18",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# for each conversation, create a message\n",
    "temp_df = df[df['file_id'] == 'M01000GE2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a2e8651-7d4d-442b-a3b3-f4348383cded",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "conversation_string = '\\n'.join(temp_df['Complete Line'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b40e544f-8c0f-4ad0-9e91-16fd9848c1ec",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "# check length, first check with just the prompt\n",
    "prompt_len = num_tokens_from_messages(messages)\n",
    "\n",
    "model_input = prompt + conversation_string\n",
    "# then check the whole convo and get the diff\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": model_input},\n",
    "]\n",
    "\n",
    "input_len = num_tokens_from_messages(messages)\n",
    "\n",
    "leftover = 4_096 - input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ee1dfeb-b3e2-4567-a2d8-e323b50f7c3f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# want capacity of about 2500 tokens for the model, which means convo must be less than\n",
    "# 4,096 - 2000 - prompt_len\n",
    "convo_target_len = 4_096 - 2250 - prompt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0324946-d8ca-45ef-86f6-d69e87fa30e1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72b35c86-c97c-43b7-bb9e-acb6000d9297",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# loop over conversations and generate complete prompts\n",
    "prompts = []\n",
    "for file_id in df['file_id'].unique():\n",
    "    file_df = df[df['file_id'] == file_id]\n",
    "\n",
    "    # identify indices where convo chunk is approx convo_target_len\n",
    "    start = 0\n",
    "    end = convo_target_len\n",
    "    idx_end = 0\n",
    "    while idx_end+1 != len(file_df):\n",
    "        chunk_df = file_df[(file_df['line_len_cumsum'] > start) & (file_df['line_len_cumsum'] <= end)]\n",
    "        idx_start = chunk_df.iloc[0].name\n",
    "        idx_end = chunk_df.iloc[-1].name\n",
    "        \n",
    "        # create model input\n",
    "        conversation_string = '\\n'.join(file_df.iloc[idx_start:idx_end+1]['Complete Line'].values)\n",
    "        model_input = prompt + conversation_string\n",
    "        \n",
    "        messages = [\n",
    "          {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\"},\n",
    "          {\"role\": \"user\", \"content\": model_input},\n",
    "        ]\n",
    "        prompts.append([file_id, messages])\n",
    "\n",
    "        # update start and end\n",
    "        start = chunk_df.iloc[-1]['line_len_cumsum']\n",
    "        end = start + convo_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8d95477-4840-465a-8ce9-a9cd25da65f2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Circumplex theory is a social psychology based theory that characterizes social interactions between speakers. The social orientation tagset includes: {Assured-Dominant, Gregarious-Extraverted, Warm-Agreeable, Unassuming-Ingenuous, Unassured-Submissive, Aloof-Introverted, Cold, Arrogant-Calculating}, which are defined below in more detail.\\n\\nAssured-Dominant - Demands to be the center of interest / Demands attention, Does most of the talking, Speaks loudly, Is firm, Is self-confident, Is forceful, Is ambitious, Is assertive, Is persistent, Is domineering, Not self-conscious\\n\\nGregarious-Extraverted - Feels comfortable around people, Starts conversations, Talks to a lot of different people, Loves large groups, Is friendly, Is enthusiastic, Is warm, Is extraverted, Is good-natured, Is cheerful / happy, Is pleasant, Is outgoing, Is approachable, Is not shy, Is \"lively\"\\n\\nWarm-Agreeable - Is interested in people, Reassures others, Inquires about others\\' well-being, Gets along well with others, Is kind, Is polite and courteous, Is sympathetic, Is respectful, Is tender-hearted, Is cooperative, Is appreciative, Is accommodating, Is gentle, Is charitable\\n\\nUnassuming-Ingenuous - Tolerates a lot from others, Takes things as they come, Tells the truth, Thinks of others first, Does not brag or boast, Seldom stretches the truth, Does not scheme or plot, Is modest, Is trustworthy, Is unassuming, Is honest, Not self-centered, Is sincere, Not demanding, Is straightforward\\n\\nUnassured-Submissive - Speaks softly, Lets others finish what they are saying, Dislikes being the center of attention, Doubts themselves, Not especially thorough, Doesn’t like to work too hard / will give up easily,  Is impractical, Is timid, Is inconsistent, Is weak, Is disorganized, Is not authoritative, Is a bit lazy, Is not forceful\\n\\nAloof-Introverted - Is quiet, especially around strangers, Is a very private person, Doesn\\'t talk a lot / Has little to say, Doesn’t smile much, Doesn’t reveal much about themselves, Is not demonstrative (verbally or non-verbally), Is distant, Is shy, Is impersonal, Is introverted, Is disinterested in others, Is bashful, Is not very social, Is focused inward\\n\\nCold - Believes people should fend for themselves, Doesn\\'t fall for sob-stories, Is not interested in other people\\'s problems, Not warm toward others, Is cruel, Is ruthless, Is cold-hearted, Is hard-hearted, Is unsympathetic, Is uncharitable\\n\\nArrogant-Calculating - Flaunts what they have, Boasts and brags, Will plot and scheme to get ahead, Willing to exploit others for own benefit, Is big-headed, Is tricky, Is boisterous, Is conniving / calculating, Is conceited, Is crafty / cunning, Is cocky, Is manipulative of others\\n---\\n\\nIn the following conversation, each line corresponds to a speaker, an utterance ID, and the text spoken. For each utterance, assign a social orientation tag, identify the utterance by its speaker and ID, and provide a brief explanation.\\n\\nSpeaker 1 (1):  你手机充300不够是吧？\\nSpeaker 2 (2):  115.51？\\nSpeaker 2 (3):  我都糊涂了\\nSpeaker 2 (4):  我打电话问问吧\\nSpeaker 1 (5):  昂\\nSpeaker 2 (6):  6月份是115块多，加上7月份的一共301块多，交305就全清了\\nSpeaker 1 (7):  您好，我现在有事不在，一会再和您联系。\\nSpeaker 1 (8):  昂\\nSpeaker 1 (9):  先充了20的快充\\nSpeaker 1 (10):  剩下的300是慢充\\nSpeaker 2 (11):  好的\\nSpeaker 1 (12):  小火车到了\\nSpeaker 2 (13):  大不，显好不\\nSpeaker 1 (14):  我又没见\\nSpeaker 2 (15):  买的多钱的来着\\nSpeaker 1 (16):  正在派件\\nSpeaker 2 (17):  周四前肯定到了\\nSpeaker 1 (18):  99\\nSpeaker 2 (19):  呃……\\nSpeaker 1 (20):  我买的还是搞活动里面最贵的\\nSpeaker 2 (21):  哦\\nSpeaker 2 (22):  那原价爱你得2，3百\\nSpeaker 1 (23):  1.8m的\\nSpeaker 1 (24):  我估计他儿玩不了\\nSpeaker 1 (25):  我选的托马斯那款\\nSpeaker 2 (26):  俺的妈呀，这么复杂啊\\nSpeaker 1 (27):  是吧\\nSpeaker 1 (28):  都是这样的\\nSpeaker 2 (29):  左旋肉碱只有一盒了\\nSpeaker 1 (30):  管事不？\\nSpeaker 2 (31):  管点事儿吧，不过最近停留在128左右了\\nSpeaker 1 (32):  哦\\nSpeaker 1 (33):  你看着办就行\\nSpeaker 2 (34):  电子照片大小怎么调整\\nSpeaker 1 (35):  美图秀秀就能改\\nSpeaker 1 (36):  我月陶菲去吃那个韩国菜了\\nSpeaker 1 (37):  你说还叫黄璐不？\\nSpeaker 2 (38):  都行啊\\nSpeaker 1 (39):  陶菲说上次宝康也给了一部分钱\\nSpeaker 1 (40):  我们是不是周四有事？\\nSpeaker 2 (41):  嗯\\nSpeaker 2 (42):  周三可以\\nSpeaker 1 (43):  你还去奥体吧？\\nSpeaker 2 (44):  不用\\nSpeaker 1 (45):  那约周三？\\nSpeaker 2 (46):  可以\\nSpeaker 2 (47):  还困啊\\nSpeaker 1 (48):  还行吧\\nSpeaker 1 (49):  再充油开发票怎么开？\\nSpeaker 2 (50):  开到一起\\nSpeaker 1 (51):  昂\\nSpeaker 1 (52):  上次的你还没要呢\\nSpeaker 2 (53):  现在不好报啊\\nSpeaker 1 (54):  嗯\\nSpeaker 1 (55):  不值当的\\nSpeaker 1 (56):  我充 的手机费到了\\nSpeaker 1 (57):  被360拦截了\\nSpeaker 1 (58):  你看看你的呢\\nSpeaker 1 (59):  一共给你充了320\\nSpeaker 2 (60):  也被拦截了，320\\nSpeaker 1 (61):  嗯\\nSpeaker 1 (62):  [图片]\\nSpeaker 1 (63):  你要是穿个这个，是不是很霸气？\\nSpeaker 1 (64):  [图片]\\nSpeaker 1 (65):  人呢？\\nSpeaker 2 (66):  昨晚我就看见了\\nSpeaker 2 (67):  刚看回去了\\nSpeaker 2 (68):  开会\\nSpeaker 1 (69):  哦\\nSpeaker 1 (70):  我去问笔记本的事情了\\nSpeaker 1 (71):  他说就是灰多了'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "529ea842-6cdb-4f5d-b77a-42c8f8df3556",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circumplex theory is a social psychology based theory that characterizes social interactions between speakers. The social orientation tagset includes: {Assured-Dominant, Gregarious-Extraverted, Warm-Agreeable, Unassuming-Ingenuous, Unassured-Submissive, Aloof-Introverted, Cold, Arrogant-Calculating}, which are defined below in more detail.\n",
      "\n",
      "Assured-Dominant - Demands to be the center of interest / Demands attention, Does most of the talking, Speaks loudly, Is firm, Is self-confident, Is forceful, Is ambitious, Is assertive, Is persistent, Is domineering, Not self-conscious\n",
      "\n",
      "Gregarious-Extraverted - Feels comfortable around people, Starts conversations, Talks to a lot of different people, Loves large groups, Is friendly, Is enthusiastic, Is warm, Is extraverted, Is good-natured, Is cheerful / happy, Is pleasant, Is outgoing, Is approachable, Is not shy, Is \"lively\"\n",
      "\n",
      "Warm-Agreeable - Is interested in people, Reassures others, Inquires about others' well-being, Gets along well with others, Is kind, Is polite and courteous, Is sympathetic, Is respectful, Is tender-hearted, Is cooperative, Is appreciative, Is accommodating, Is gentle, Is charitable\n",
      "\n",
      "Unassuming-Ingenuous - Tolerates a lot from others, Takes things as they come, Tells the truth, Thinks of others first, Does not brag or boast, Seldom stretches the truth, Does not scheme or plot, Is modest, Is trustworthy, Is unassuming, Is honest, Not self-centered, Is sincere, Not demanding, Is straightforward\n",
      "\n",
      "Unassured-Submissive - Speaks softly, Lets others finish what they are saying, Dislikes being the center of attention, Doubts themselves, Not especially thorough, Doesn’t like to work too hard / will give up easily,  Is impractical, Is timid, Is inconsistent, Is weak, Is disorganized, Is not authoritative, Is a bit lazy, Is not forceful\n",
      "\n",
      "Aloof-Introverted - Is quiet, especially around strangers, Is a very private person, Doesn't talk a lot / Has little to say, Doesn’t smile much, Doesn’t reveal much about themselves, Is not demonstrative (verbally or non-verbally), Is distant, Is shy, Is impersonal, Is introverted, Is disinterested in others, Is bashful, Is not very social, Is focused inward\n",
      "\n",
      "Cold - Believes people should fend for themselves, Doesn't fall for sob-stories, Is not interested in other people's problems, Not warm toward others, Is cruel, Is ruthless, Is cold-hearted, Is hard-hearted, Is unsympathetic, Is uncharitable\n",
      "\n",
      "Arrogant-Calculating - Flaunts what they have, Boasts and brags, Will plot and scheme to get ahead, Willing to exploit others for own benefit, Is big-headed, Is tricky, Is boisterous, Is conniving / calculating, Is conceited, Is crafty / cunning, Is cocky, Is manipulative of others\n",
      "---\n",
      "\n",
      "In the following conversation, each line corresponds to a speaker, an utterance ID, and the text spoken. For each utterance, assign a social orientation tag, identify the utterance by its speaker and ID, and provide a brief explanation.\n",
      "\n",
      "Speaker 1 (1):  你手机充300不够是吧？\n",
      "Speaker 2 (2):  115.51？\n",
      "Speaker 2 (3):  我都糊涂了\n",
      "Speaker 2 (4):  我打电话问问吧\n",
      "Speaker 1 (5):  昂\n",
      "Speaker 2 (6):  6月份是115块多，加上7月份的一共301块多，交305就全清了\n",
      "Speaker 1 (7):  您好，我现在有事不在，一会再和您联系。\n",
      "Speaker 1 (8):  昂\n",
      "Speaker 1 (9):  先充了20的快充\n",
      "Speaker 1 (10):  剩下的300是慢充\n",
      "Speaker 2 (11):  好的\n",
      "Speaker 1 (12):  小火车到了\n",
      "Speaker 2 (13):  大不，显好不\n",
      "Speaker 1 (14):  我又没见\n",
      "Speaker 2 (15):  买的多钱的来着\n",
      "Speaker 1 (16):  正在派件\n",
      "Speaker 2 (17):  周四前肯定到了\n",
      "Speaker 1 (18):  99\n",
      "Speaker 2 (19):  呃……\n",
      "Speaker 1 (20):  我买的还是搞活动里面最贵的\n",
      "Speaker 2 (21):  哦\n",
      "Speaker 2 (22):  那原价爱你得2，3百\n",
      "Speaker 1 (23):  1.8m的\n",
      "Speaker 1 (24):  我估计他儿玩不了\n",
      "Speaker 1 (25):  我选的托马斯那款\n",
      "Speaker 2 (26):  俺的妈呀，这么复杂啊\n",
      "Speaker 1 (27):  是吧\n",
      "Speaker 1 (28):  都是这样的\n",
      "Speaker 2 (29):  左旋肉碱只有一盒了\n",
      "Speaker 1 (30):  管事不？\n",
      "Speaker 2 (31):  管点事儿吧，不过最近停留在128左右了\n",
      "Speaker 1 (32):  哦\n",
      "Speaker 1 (33):  你看着办就行\n",
      "Speaker 2 (34):  电子照片大小怎么调整\n",
      "Speaker 1 (35):  美图秀秀就能改\n",
      "Speaker 1 (36):  我月陶菲去吃那个韩国菜了\n",
      "Speaker 1 (37):  你说还叫黄璐不？\n",
      "Speaker 2 (38):  都行啊\n",
      "Speaker 1 (39):  陶菲说上次宝康也给了一部分钱\n",
      "Speaker 1 (40):  我们是不是周四有事？\n",
      "Speaker 2 (41):  嗯\n",
      "Speaker 2 (42):  周三可以\n",
      "Speaker 1 (43):  你还去奥体吧？\n",
      "Speaker 2 (44):  不用\n",
      "Speaker 1 (45):  那约周三？\n",
      "Speaker 2 (46):  可以\n",
      "Speaker 2 (47):  还困啊\n",
      "Speaker 1 (48):  还行吧\n",
      "Speaker 1 (49):  再充油开发票怎么开？\n",
      "Speaker 2 (50):  开到一起\n",
      "Speaker 1 (51):  昂\n",
      "Speaker 1 (52):  上次的你还没要呢\n",
      "Speaker 2 (53):  现在不好报啊\n",
      "Speaker 1 (54):  嗯\n",
      "Speaker 1 (55):  不值当的\n",
      "Speaker 1 (56):  我充 的手机费到了\n",
      "Speaker 1 (57):  被360拦截了\n",
      "Speaker 1 (58):  你看看你的呢\n",
      "Speaker 1 (59):  一共给你充了320\n",
      "Speaker 2 (60):  也被拦截了，320\n",
      "Speaker 1 (61):  嗯\n",
      "Speaker 1 (62):  [图片]\n",
      "Speaker 1 (63):  你要是穿个这个，是不是很霸气？\n",
      "Speaker 1 (64):  [图片]\n",
      "Speaker 1 (65):  人呢？\n",
      "Speaker 2 (66):  昨晚我就看见了\n",
      "Speaker 2 (67):  刚看回去了\n",
      "Speaker 2 (68):  开会\n",
      "Speaker 1 (69):  哦\n",
      "Speaker 1 (70):  我去问笔记本的事情了\n",
      "Speaker 1 (71):  他说就是灰多了\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0][-1][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8d350f9-ca5c-400a-9243-f5217fe64b00",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# estimate cost\n",
    "token_count = 0\n",
    "for p in prompts:\n",
    "    token_count += num_tokens_from_messages(p[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f777fffc-0fb2-49f4-9c66-16a70f08e99e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.674286"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# price\n",
    "(token_count / 1000) * 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3fb8f282-52b2-4a24-afc4-8a3a5d2b7ce9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "# filename = \"data/gpt_requests.jsonl\"\n",
    "\n",
    "# with open(filename, \"w\") as f:\n",
    "#     for p in prompts:\n",
    "#         json_string = json.dumps(p)\n",
    "#         f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9130e61c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['filename'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e297c6c6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34558"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8c9c435",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R1           20864\n",
       "Mini-Eval    13694\n",
       "Name: release, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.merge(meta_df.drop_duplicates(subset=['file_uid']), left_on='file_id', right_on='file_uid', how='left')['release'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9913cec3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>@id</th>\n",
       "      <th>@start_char</th>\n",
       "      <th>@end_char</th>\n",
       "      <th>ORIGINAL_TEXT</th>\n",
       "      <th>@type</th>\n",
       "      <th>@begin_offset</th>\n",
       "      <th>@char_length</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>social_orientation</th>\n",
       "      <th>file_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>impact_scalar</th>\n",
       "      <th>comment</th>\n",
       "      <th>Utterance ID</th>\n",
       "      <th>Complete Line</th>\n",
       "      <th>Complete Line Length</th>\n",
       "      <th>line_len_cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>你手机充300不够是吧？</td>\n",
       "      <td>message</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>m0000</td>\n",
       "      <td>2012-07-09 08:54:39 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-0-0', '@pos': 'word', '@morph'...</td>\n",
       "      <td>Cold</td>\n",
       "      <td>M01000EY0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Speaker 1 (1):  你手机充300不够是吧？</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-1</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>115.51？</td>\n",
       "      <td>message</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>m0001</td>\n",
       "      <td>2012-07-09 08:56:50 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-1-0', '@pos': 'word', '@morph'...</td>\n",
       "      <td>Warm-Agreeable</td>\n",
       "      <td>M01000EY0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Speaker 2 (2):  115.51？</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-2</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>我都糊涂了</td>\n",
       "      <td>message</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>m0002</td>\n",
       "      <td>2012-07-09 08:56:53 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-2-0', '@pos': 'word', '@morph'...</td>\n",
       "      <td>Arrogant-Calculating</td>\n",
       "      <td>M01000EY0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Speaker 2 (3):  我都糊涂了</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-3</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>我打电话问问吧</td>\n",
       "      <td>message</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>m0003</td>\n",
       "      <td>2012-07-09 08:56:59 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-3-0', '@pos': 'word', '@morph'...</td>\n",
       "      <td>Cold</td>\n",
       "      <td>M01000EY0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Speaker 2 (4):  我打电话问问吧</td>\n",
       "      <td>15</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M01000EY0.ltf.xml</td>\n",
       "      <td>segment-4</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>昂</td>\n",
       "      <td>message</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>m0004</td>\n",
       "      <td>2012-07-09 08:57:56 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@id': 'token-4-0', '@pos': 'word', '@morph':...</td>\n",
       "      <td>Cold</td>\n",
       "      <td>M01000EY0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Speaker 1 (5):  昂</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>M01000HU5.ltf.xml</td>\n",
       "      <td>segment-96</td>\n",
       "      <td>1042</td>\n",
       "      <td>1048</td>\n",
       "      <td>哪个企业？你妹</td>\n",
       "      <td>message</td>\n",
       "      <td>1042</td>\n",
       "      <td>9</td>\n",
       "      <td>m0096</td>\n",
       "      <td>2012-11-02 23:34:43 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-96-0', '@pos': 'word', '@morph...</td>\n",
       "      <td>Assured-Dominant</td>\n",
       "      <td>M01000HU5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>Speaker 2 (97):  哪个企业？你妹</td>\n",
       "      <td>18</td>\n",
       "      <td>1816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>M01000HU5.ltf.xml</td>\n",
       "      <td>segment-97</td>\n",
       "      <td>1051</td>\n",
       "      <td>1052</td>\n",
       "      <td>腐败</td>\n",
       "      <td>message</td>\n",
       "      <td>1051</td>\n",
       "      <td>4</td>\n",
       "      <td>m0097</td>\n",
       "      <td>2012-11-02 23:35:31 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@id': 'token-97-0', '@pos': 'word', '@morph'...</td>\n",
       "      <td>Unassuming-Ingenuous</td>\n",
       "      <td>M01000HU5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>Speaker 2 (98):  腐败</td>\n",
       "      <td>11</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>M01000HU5.ltf.xml</td>\n",
       "      <td>segment-98</td>\n",
       "      <td>1055</td>\n",
       "      <td>1062</td>\n",
       "      <td>想腐败都没有机会</td>\n",
       "      <td>message</td>\n",
       "      <td>1055</td>\n",
       "      <td>10</td>\n",
       "      <td>m0098</td>\n",
       "      <td>2012-11-02 23:40:47 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-98-0', '@pos': 'word', '@morph...</td>\n",
       "      <td>Unassured-Submissive</td>\n",
       "      <td>M01000HU5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>Speaker 1 (99):  想腐败都没有机会</td>\n",
       "      <td>18</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>M01000HU5.ltf.xml</td>\n",
       "      <td>segment-99</td>\n",
       "      <td>1065</td>\n",
       "      <td>1066</td>\n",
       "      <td>就是</td>\n",
       "      <td>message</td>\n",
       "      <td>1065</td>\n",
       "      <td>4</td>\n",
       "      <td>m0099</td>\n",
       "      <td>2012-11-02 23:41:51 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@id': 'token-99-0', '@pos': 'word', '@morph'...</td>\n",
       "      <td>Warm-Agreeable</td>\n",
       "      <td>M01000HU5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>Speaker 2 (100):  就是</td>\n",
       "      <td>10</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>M01000HU5.ltf.xml</td>\n",
       "      <td>segment-100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1072</td>\n",
       "      <td>屌丝的命</td>\n",
       "      <td>message</td>\n",
       "      <td>1069</td>\n",
       "      <td>6</td>\n",
       "      <td>m0100</td>\n",
       "      <td>2012-11-02 23:41:54 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'@id': 'token-100-0', '@pos': 'word', '@morp...</td>\n",
       "      <td>Gregarious-Extraverted</td>\n",
       "      <td>M01000HU5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>Speaker 2 (101):  屌丝的命</td>\n",
       "      <td>13</td>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34558 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename          @id @start_char @end_char ORIGINAL_TEXT  \\\n",
       "0    M01000EY0.ltf.xml    segment-0           0        11  你手机充300不够是吧？   \n",
       "1    M01000EY0.ltf.xml    segment-1          14        20       115.51？   \n",
       "2    M01000EY0.ltf.xml    segment-2          23        27         我都糊涂了   \n",
       "3    M01000EY0.ltf.xml    segment-3          30        36       我打电话问问吧   \n",
       "4    M01000EY0.ltf.xml    segment-4          39        39             昂   \n",
       "..                 ...          ...         ...       ...           ...   \n",
       "96   M01000HU5.ltf.xml   segment-96        1042      1048       哪个企业？你妹   \n",
       "97   M01000HU5.ltf.xml   segment-97        1051      1052            腐败   \n",
       "98   M01000HU5.ltf.xml   segment-98        1055      1062      想腐败都没有机会   \n",
       "99   M01000HU5.ltf.xml   segment-99        1065      1066            就是   \n",
       "100  M01000HU5.ltf.xml  segment-100        1069      1072          屌丝的命   \n",
       "\n",
       "       @type  @begin_offset  @char_length     id                     time  \\\n",
       "0    message              0            14  m0000  2012-07-09 08:54:39 UTC   \n",
       "1    message             14             9  m0001  2012-07-09 08:56:50 UTC   \n",
       "2    message             23             7  m0002  2012-07-09 08:56:53 UTC   \n",
       "3    message             30             9  m0003  2012-07-09 08:56:59 UTC   \n",
       "4    message             39             3  m0004  2012-07-09 08:57:56 UTC   \n",
       "..       ...            ...           ...    ...                      ...   \n",
       "96   message           1042             9  m0096  2012-11-02 23:34:43 UTC   \n",
       "97   message           1051             4  m0097  2012-11-02 23:35:31 UTC   \n",
       "98   message           1055            10  m0098  2012-11-02 23:40:47 UTC   \n",
       "99   message           1065             4  m0099  2012-11-02 23:41:51 UTC   \n",
       "100  message           1069             6  m0100  2012-11-02 23:41:54 UTC   \n",
       "\n",
       "     ...                                              TOKEN  \\\n",
       "0    ...  [{'@id': 'token-0-0', '@pos': 'word', '@morph'...   \n",
       "1    ...  [{'@id': 'token-1-0', '@pos': 'word', '@morph'...   \n",
       "2    ...  [{'@id': 'token-2-0', '@pos': 'word', '@morph'...   \n",
       "3    ...  [{'@id': 'token-3-0', '@pos': 'word', '@morph'...   \n",
       "4    ...  {'@id': 'token-4-0', '@pos': 'word', '@morph':...   \n",
       "..   ...                                                ...   \n",
       "96   ...  [{'@id': 'token-96-0', '@pos': 'word', '@morph...   \n",
       "97   ...  {'@id': 'token-97-0', '@pos': 'word', '@morph'...   \n",
       "98   ...  [{'@id': 'token-98-0', '@pos': 'word', '@morph...   \n",
       "99   ...  {'@id': 'token-99-0', '@pos': 'word', '@morph'...   \n",
       "100  ...  [{'@id': 'token-100-0', '@pos': 'word', '@morp...   \n",
       "\n",
       "         social_orientation    file_id timestamp  impact_scalar  comment  \\\n",
       "0                      Cold  M01000EY0       NaN            NaN      NaN   \n",
       "1            Warm-Agreeable  M01000EY0       NaN            NaN      NaN   \n",
       "2      Arrogant-Calculating  M01000EY0       NaN            NaN      NaN   \n",
       "3                      Cold  M01000EY0       NaN            NaN      NaN   \n",
       "4                      Cold  M01000EY0       NaN            NaN      NaN   \n",
       "..                      ...        ...       ...            ...      ...   \n",
       "96         Assured-Dominant  M01000HU5       NaN            NaN      NaN   \n",
       "97     Unassuming-Ingenuous  M01000HU5       NaN            NaN      NaN   \n",
       "98     Unassured-Submissive  M01000HU5       NaN            NaN      NaN   \n",
       "99           Warm-Agreeable  M01000HU5       NaN            NaN      NaN   \n",
       "100  Gregarious-Extraverted  M01000HU5       NaN            NaN      NaN   \n",
       "\n",
       "    Utterance ID                 Complete Line Complete Line Length  \\\n",
       "0              1  Speaker 1 (1):  你手机充300不够是吧？                   20   \n",
       "1              2       Speaker 2 (2):  115.51？                   12   \n",
       "2              3         Speaker 2 (3):  我都糊涂了                   16   \n",
       "3              4       Speaker 2 (4):  我打电话问问吧                   15   \n",
       "4              5             Speaker 1 (5):  昂                    9   \n",
       "..           ...                           ...                  ...   \n",
       "96            97      Speaker 2 (97):  哪个企业？你妹                   18   \n",
       "97            98           Speaker 2 (98):  腐败                   11   \n",
       "98            99     Speaker 1 (99):  想腐败都没有机会                   18   \n",
       "99           100          Speaker 2 (100):  就是                   10   \n",
       "100          101        Speaker 2 (101):  屌丝的命                   13   \n",
       "\n",
       "     line_len_cumsum  \n",
       "0                 20  \n",
       "1                 32  \n",
       "2                 48  \n",
       "3                 63  \n",
       "4                 72  \n",
       "..               ...  \n",
       "96              1816  \n",
       "97              1827  \n",
       "98              1845  \n",
       "99              1855  \n",
       "100             1868  \n",
       "\n",
       "[34558 rows x 21 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b7486ae",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# save the final df to disk\n",
    "df.rename(columns={'social_orientation': 'social_orientation_random'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20311126",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "circumplex_dir = os.path.join(data_dir, 'transformed/circumplex')\n",
    "os.makedirs(circumplex_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "719aed2c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "save_filepath = os.path.join(circumplex_dir, 'gpt_prompts_r1_mini_eval_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d2e733d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(save_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c21d3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# run GPT on the prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CHARM)",
   "language": "python",
   "name": "charm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
